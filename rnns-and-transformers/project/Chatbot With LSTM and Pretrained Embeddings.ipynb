{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Steps Overview and Estimated Duration\n",
    "# Below you will find each of the components of the project, and estimated times to complete each portion. \n",
    "# These are estimates and not exact timings to help you expect the amount of time necessary to put aside to work on your project.\n",
    "\n",
    "# Prepare data (~2 hours)\n",
    "# Build your vocabulary from a corpus of language data. The Vocabulary object is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Build Model (~4 hours)\n",
    "# Build your Encoder, Decoder, and larger Sequence to Sequence pattern in PyTorch. This pattern is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Train Model (~3 hours)\n",
    "# Write your training procedure and divide your dataset into train/test/validation splits. Then, train your network and plot your evaluation metrics. Save your model after it reaches a satisfactory level of accuracy.\n",
    "\n",
    "# Evaluate & Interact w/ Model (~1 hour)\n",
    "# Write a script to interact with your network at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Summary\n",
    "# The LSTM Chatbot will help you show off your skills as a deep learning practitioner. You will develop the chatbot using a new architecture called a Seq2Seq. \n",
    "# Additionally, you can use pre-trained word embeddings to improve the performance of your model. Let's get started by following the steps below:\n",
    "\n",
    "# Step 1: Build your Vocabulary & create the Word Embeddings\n",
    "# The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Use Gensim to extract the word embeddings from one of its corpus'.\n",
    "# Use NLTK and Gensim to create a function to clean your text and look up the index of a word's embeddings.\n",
    "\n",
    "# Step 2: Create the Encoder\n",
    "# A Seq2Seq architecture consists of an encoder and a decoder unit. You will use Pytorch to build a full Seq2Seq model.\n",
    "# The first step of the architecture is to create an encoder with an LSTM unit.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Load your pretrained embeddings into the LSTM unit.\n",
    "\n",
    "# Step 3: Create the Decoder\n",
    "# The second step of the architecture is to create a decoder using a second LSTM unit.\n",
    "\n",
    "# Step 4: Combine them into a Seq2Seq Architecture\n",
    "# To finalize your model, you will combine the encoder and decoder units into a working model.\n",
    "# The Seq2Seq2 model must be able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function.\n",
    "\n",
    "# Step 5: Train & evaluate your model\n",
    "# Finally you will train and evaluate your model using a Pytorch training loop.\n",
    "\n",
    "# Step 6: Interact with the Chatbot\n",
    "# Demonstrate your chatbot by converting the outputs of the model to text and displaying it's responses at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pre-requisites: \n",
    "# - PyTorch 2.00 kernel\n",
    "# - ml.g5.xlarge instance\n",
    "\n",
    "# Install requirements\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1 torchtext torchdata portalocker | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.15.1 portalocker>=2.0.0 | grep -v \"already satisfied\"\n",
    "!pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.6.0   | grep -v \"already satisfied\"\n",
    "\n",
    "#  torchtext==0.12.0 --> torch==1.11.0\n",
    "#  torchtext==0.13.0 --> torch==1.12.0\n",
    "#  torchtext==0.14.0 --> torch==1.12.0\n",
    "# torchtext==0.15.1 --> torch==2.0.0\n",
    "# torchtext==0.15.2 --> torch==2.0.1\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.10.0 | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.2.0 nltk torchtext  | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import sklearn.model_selection \n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "question_context_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "answer_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_text(sentence):\n",
    "    return sentence\n",
    "\n",
    "def loadDF(path):    \n",
    "    data_file_path = download_from_url(path, root=\"data\")\n",
    "    \n",
    "    with open(data_file_path, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    examples = []\n",
    "    for item in squad_data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']                \n",
    "                # question_context = f\"{question} {separator_token} {context}\"\n",
    "                question_context = f\"{question} {context}\"\n",
    "                answer = qa['answers'][0]['text']\n",
    "                \n",
    "                data.append((question_context, answer))                                \n",
    "                \n",
    "    df = pd.DataFrame.from_records(data, columns=['question_context', 'answer'])        \n",
    "    df['question_context'] = df['question_context'].apply(prepare_text)\n",
    "    df['answer'] = df['answer'].apply(prepare_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_datasets(df, random_seed=42):\n",
    "    df.to_csv(\"data/data.csv\", index=False)\n",
    "        \n",
    "    fields = [('question_context', question_context_field), ('answer', answer_field)]\n",
    "    dataset = TabularDataset(\"data/data.csv\", format='csv', fields=fields)\n",
    "    \n",
    "    question_context_field.build_vocab(dataset, min_freq=1)\n",
    "    answer_field.build_vocab(dataset, min_freq=1)\n",
    "\n",
    "    train_data, valid_data = dataset.split(split_ratio=0.8, random_state=random.seed(random_seed))\n",
    "    \n",
    "    return train_data, valid_data\n",
    "\n",
    "\n",
    "def decode_answer(tokens):\n",
    "    return tokens_to_string(tokens, answer_field)\n",
    "\n",
    "def encode_question(question_string):\n",
    "    return string_to_tensor(question_string.lower(), question_context_field)\n",
    "    \n",
    "def tokens_to_string(tokens, field):\n",
    "    eos_idx = field.vocab.stoi[field.eos_token]\n",
    "    \n",
    "    return \" \".join([field.vocab.itos[token] for token in tokens if token != eos_idx])\n",
    "\n",
    "def string_to_tensor(string, field):\n",
    "    tokens = field.tokenize(string)\n",
    "\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>'] \n",
    "\n",
    "    token_ids = field.numericalize([tokens])\n",
    "    \n",
    "    return token_ids\n",
    "\n",
    "    # tokens_to_string(token_ids.squeeze().tolist(), question_context_field)    \n",
    "    \n",
    "class PrintToBoth:\n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, 'w')\n",
    "\n",
    "    def print(self, s):\n",
    "        print(s)\n",
    "        print(s, file=self.file, flush=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = loadDF(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "\n",
    "train_data, valid_data = load_datasets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102396\n",
      "42794\n"
     ]
    }
   ],
   "source": [
    "print(len(question_context_field.vocab))\n",
    "\n",
    "print(len(answer_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Buildi...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame? Architectura...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>What was Yangon previously known as? Kathmandu...</td>\n",
       "      <td>Rangoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>What is KMC an initialism of? Kathmandu Metrop...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_context   \n",
       "0      To whom did the Virgin Mary allegedly appear i...  \\\n",
       "1      What is in front of the Notre Dame Main Buildi...   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3      What is the Grotto at Notre Dame? Architectura...   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595  What was Yangon previously known as? Kathmandu...   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598  What is KMC an initialism of? Kathmandu Metrop...   \n",
       "\n",
       "                                        answer  \n",
       "0                   Saint Bernadette Soubirous  \n",
       "1                    a copper statue of Christ  \n",
       "2                            the Main Building  \n",
       "3      a Marian place of prayer and reflection  \n",
       "4           a golden statue of the Virgin Mary  \n",
       "...                                        ...  \n",
       "87594                                   Oregon  \n",
       "87595                                  Rangoon  \n",
       "87596                                    Minsk  \n",
       "87597                                     1975  \n",
       "87598              Kathmandu Metropolitan City  \n",
       "\n",
       "[87599 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What entity provides help with the management of time for new students at Notre Dame? All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\n",
      "Learning Resource Center\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[20]['question_context'])\n",
    "\n",
    "print(df.iloc[20]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oQLTP2Wmi1eB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)        # How to use it ????\n",
    "        # self.embedding_dim = embedding_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        embedded = self.embedding(i)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "                \n",
    "        return output, hidden, cell\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        # self.embedding = nn.Embedding(self.hidden_size, self.hidden_size)  # From lesson\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.embedding = nn.Embedding(hidden_size, embedding_size) # Why ?\n",
    "        \n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # print('[Decoder] input shape:', input.shape)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell)) \n",
    "        \n",
    "        \n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        \n",
    "        # print(\"prediction\", prediction)\n",
    "                \n",
    "        return prediction, hidden, cell\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):              \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "                \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        input = trg[0, :]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # get highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # print(\"Most likely token: \", top1)\n",
    "            \n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            input = trg[t] if use_teacher_forcing else top1\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Seq2SeqInference(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder, answer_field):\n",
    "        \n",
    "        super(Seq2SeqInference, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, max_length=20):\n",
    "        #src = [src len, 1]\n",
    "        #trg = [trg len, 1]\n",
    "        # expected output shape = [max_length, 1]\n",
    "        \n",
    "        if src.shape[1] != 1:\n",
    "            raise ValueError(f\"src.shape[1] != 1: {src.shape[1]}\")\n",
    "            \n",
    "        batch_size = 1 # src.shape[1]\n",
    "\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                                   \n",
    "        logits = torch.zeros(max_length + 1, batch_size, trg_vocab_size).to(device)\n",
    "        # outputs = torch.zeros(max_length + 1, batch_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        # TODO: create array of <sos> tokens      \n",
    "        sos_idx = answer_field.vocab.stoi[answer_field.init_token] \n",
    "        \n",
    "        # input = trg[0, :]\n",
    "        \n",
    "        # print(\"input shape\", trg.shape)\n",
    "        input = torch.tensor([sos_idx]).to(device)        \n",
    "        # print(\"Start token: \", input)\n",
    "        \n",
    "        inferred_tokens = []\n",
    "                \n",
    "        # for t in range(1, trg_len):\n",
    "        for t in range(1, max_length + 1):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            logits[t] = output\n",
    "            \n",
    "            # Get highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # print(f\"Top1: {top1} {top1.item()}\")\n",
    "            # print(f\"Most likely token: {top1.item()} ({answer_field.vocab.itos[top1.item()]})\")\n",
    "            inferred_tokens.append(top1)\n",
    "                        \n",
    "            input = top1                        \n",
    "        \n",
    "        inferred_tokens_tensor = torch.tensor(inferred_tokens).view(-1, 1)\n",
    "        \n",
    "        logits_dim = logits.shape[-1]\n",
    "        logits = logits[1:].view(-1, logits_dim)\n",
    "        \n",
    "        return inferred_tokens_tensor, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(102396, 468)\n",
       "    (lstm): LSTM(468, 938, dropout=0.6039111450304973)\n",
       "    (dropout): Dropout(p=0.6039111450304973, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(42794, 256)\n",
       "    (lstm): LSTM(256, 938, dropout=0.33154046338693915)\n",
       "    (fc): Linear(in_features=938, out_features=42794, bias=True)\n",
       "    (dropout): Dropout(p=0.33154046338693915, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Best hyperparameters: {'encoder_dropout': 0.6039111450304973, 'decoder_dropout': 0.33154046338693915, 'embedding_size': 468, 'hidden_size': 938, 'num_layers': 1, 'momentum': 0.5494925017261035, 'learning_rate': 0.04835390488901119, 'weight_decay': 0.00014738274625279338, 'batch_size': 132}\n",
    "# Validation loss: 6.840339520820101\n",
    "\n",
    "# Define the model and other parameters\n",
    "encoder_input_size = len(question_context_field.vocab) \n",
    "encoder_embedding_size= 468 #256 \n",
    "encoder_dropout = 0.6039111450304973 #0.1 \n",
    "\n",
    "decoder_output_size = len(answer_field.vocab)\n",
    "decoder_embedding_size = 256 #362 #300\n",
    "decoder_dropout = 0.33154046338693915 #0.1 \n",
    "\n",
    "hidden_size = 938 #256 #512 #1602\n",
    "num_layers = 1\n",
    "batch_size = 132 #64 #85 #128\n",
    "\n",
    "learning_rate = 0.04835390488901119 #0.003 \n",
    "weight_decay= 0.00014738274625279338 #1e-3\n",
    "momentum= 0.5494925017261035 #0.5\n",
    "\n",
    "num_epochs =  100\n",
    "\n",
    "encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "   batch_size=batch_size,\n",
    "   sort_within_batch=True,\n",
    "    sort_key = lambda x: len(x.question_context),\n",
    "    device=device)\n",
    "\n",
    "\n",
    "# Init model weights\n",
    "for name, param in model.named_parameters():\n",
    "    nn.init.uniform_(param.data, -0.08, 0.08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum validation loss: 7.164206. Saving model ...\n",
      "Epoch: 01, Train Loss: 7.660, Val Loss: 7.164 Train Time: 188.57793474197388s Eval Time: 20.22123908996582s\n",
      "New minimum validation loss: 6.980491. Saving model ...\n",
      "Epoch: 02, Train Loss: 7.049, Val Loss: 6.980 Train Time: 187.96233129501343s Eval Time: 20.14483880996704s\n",
      "New minimum validation loss: 6.890448. Saving model ...\n",
      "Epoch: 03, Train Loss: 6.922, Val Loss: 6.890 Train Time: 189.05081629753113s Eval Time: 20.188480854034424s\n",
      "Epoch: 04, Train Loss: 6.864, Val Loss: 6.863 Train Time: 189.4222068786621s Eval Time: 20.15251111984253s\n",
      "Epoch: 05, Train Loss: 6.825, Val Loss: 6.844 Train Time: 189.15589547157288s Eval Time: 20.124694347381592s\n",
      "Epoch: 06, Train Loss: 6.800, Val Loss: 6.846 Train Time: 189.11357736587524s Eval Time: 20.227267742156982s\n",
      "New minimum validation loss: 6.820378. Saving model ...\n",
      "Epoch: 07, Train Loss: 6.783, Val Loss: 6.820 Train Time: 188.4856460094452s Eval Time: 20.12617588043213s\n",
      "Epoch: 08, Train Loss: 6.766, Val Loss: 6.812 Train Time: 189.14682745933533s Eval Time: 20.13550901412964s\n",
      "Epoch: 09, Train Loss: 6.753, Val Loss: 6.808 Train Time: 188.4538130760193s Eval Time: 20.141372203826904s\n",
      "Epoch: 10, Train Loss: 6.736, Val Loss: 6.813 Train Time: 188.61356925964355s Eval Time: 20.340046644210815s\n",
      "Epoch: 11, Train Loss: 6.723, Val Loss: 6.814 Train Time: 189.13922119140625s Eval Time: 20.22509455680847s\n",
      "Epoch: 12, Train Loss: 6.713, Val Loss: 6.812 Train Time: 187.90590047836304s Eval Time: 20.264652729034424s\n",
      "Epoch 00013: reducing learning rate of group 0 to 4.8354e-03.\n",
      "Epoch: 13, Train Loss: 6.702, Val Loss: 6.809 Train Time: 188.4783113002777s Eval Time: 20.236758708953857s\n",
      "Epoch: 14, Train Loss: 6.661, Val Loss: 6.794 Train Time: 192.96250653266907s Eval Time: 21.29325795173645s\n",
      "Epoch: 15, Train Loss: 6.658, Val Loss: 6.795 Train Time: 191.8389995098114s Eval Time: 20.785841941833496s\n",
      "Epoch: 16, Train Loss: 6.657, Val Loss: 6.793 Train Time: 190.56915068626404s Eval Time: 20.569652318954468s\n",
      "Epoch: 17, Train Loss: 6.656, Val Loss: 6.794 Train Time: 189.33202648162842s Eval Time: 20.270973205566406s\n",
      "Epoch: 18, Train Loss: 6.652, Val Loss: 6.795 Train Time: 189.19817423820496s Eval Time: 20.273013591766357s\n",
      "Epoch 00019: reducing learning rate of group 0 to 4.8354e-04.\n",
      "Epoch: 19, Train Loss: 6.652, Val Loss: 6.796 Train Time: 190.3218355178833s Eval Time: 20.304402589797974s\n",
      "Epoch: 20, Train Loss: 6.649, Val Loss: 6.794 Train Time: 186.55456280708313s Eval Time: 20.17399764060974s\n",
      "Epoch: 21, Train Loss: 6.650, Val Loss: 6.794 Train Time: 189.37754011154175s Eval Time: 20.297173738479614s\n",
      "Epoch: 22, Train Loss: 6.649, Val Loss: 6.794 Train Time: 187.8747203350067s Eval Time: 20.284196376800537s\n",
      "Epoch: 23, Train Loss: 6.650, Val Loss: 6.794 Train Time: 187.7808825969696s Eval Time: 20.274154901504517s\n",
      "Epoch: 24, Train Loss: 6.648, Val Loss: 6.794 Train Time: 188.86425161361694s Eval Time: 20.259441614151s\n",
      "Epoch 00025: reducing learning rate of group 0 to 4.8354e-05.\n",
      "Epoch: 25, Train Loss: 6.650, Val Loss: 6.794 Train Time: 190.1567633152008s Eval Time: 20.40457320213318s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# print (src.shape)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print (trg.shape)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Pass the source sequences through the encoder\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m#  [trg length, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print('output:', output.shape)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 95\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     92\u001b[0m trg_len \u001b[38;5;241m=\u001b[39m trg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m trg_vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39moutput_size\n\u001b[0;32m---> 95\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_vocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     97\u001b[0m _, hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Start with <sos> tokens\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "        \n",
    "now = datetime.datetime.now()\n",
    "train_output_filename = f'train_output_{now.strftime(\"%Y%m%d_%H%M%S\")}.txt'        \n",
    "tee_print = PrintToBoth(train_output_filename)       \n",
    "\n",
    "# learning_rate = 0.001\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.01) # YOUR CODE HERE\n",
    "\n",
    "# num_epochs =  20\n",
    "clip = 5\n",
    "valid_loss_min = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    avg_train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "        \n",
    "#         print('src:', src.shape)\n",
    "#         print('trg:', trg.shape)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print (src.shape)\n",
    "        # print (trg.shape)\n",
    "        \n",
    "        # Pass the source sequences through the encoder\n",
    "        output = model(src, trg)        #  [trg length, batch size, output dim]\n",
    "        # print('output:', output.shape)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "#         print('output 2:', output.shape)\n",
    "#         print('trg 2:', trg.shape)\n",
    "        \n",
    "#         print('output content:', output)\n",
    "#         print('trg content:', trg)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    average_train_loss = train_loss / len(train_iterator)\n",
    "    \n",
    "    end_time = time.time()    \n",
    "    train_elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Evaluation\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            src = batch.question_context.to(device)\n",
    "            trg = batch.answer.to(device)                    \n",
    "            \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "        average_val_loss = valid_loss / len(valid_iterator)\n",
    "                    \n",
    "    end_time = time.time()    \n",
    "    val_elapsed_time = end_time - start_time\n",
    "    \n",
    "    # print(f'Eval Time: {elapsed_time}')\n",
    "    # print(f'Epoch: {epoch+1:02} | Eval Time: {val_elapsed_time}s')\n",
    "    scheduler.step(average_val_loss)    \n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, f'checkpoints/train-{epoch}.pt')\n",
    "    \n",
    "    if valid_loss_min is None or (\n",
    "            (valid_loss_min - average_val_loss) / valid_loss_min > 0.01   # valid_loss_min - 0.01 * valid_loss_min    > average_val_loss\n",
    "    ):\n",
    "        # print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "        tee_print.print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, 'checkpoints/best_val_loss.pt')\n",
    "\n",
    "        valid_loss_min = average_val_loss\n",
    "        \n",
    "    tee_print.print(f\"Epoch: {epoch:02}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f} Train Time: {train_elapsed_time}s Eval Time: {val_elapsed_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New minimum validation loss: 9.777593. Saving model ...\n",
    "# Epoch: 01, Train Loss: 10.325, Val Loss: 9.778 Train Time: 138.35026669502258s Eval Time: 15.636022806167603s\n",
    "# New minimum validation loss: 8.429811. Saving model ...\n",
    "# Epoch: 02, Train Loss: 8.829, Val Loss: 8.430 Train Time: 137.56066799163818s Eval Time: 15.643161058425903s\n",
    "# New minimum validation loss: 8.051929. Saving model ...\n",
    "# Epoch: 03, Train Loss: 8.262, Val Loss: 8.052 Train Time: 137.84890174865723s Eval Time: 15.51107907295227s\n",
    "# New minimum validation loss: 7.829260. Saving model ...\n",
    "# Epoch: 04, Train Loss: 7.918, Val Loss: 7.829 Train Time: 138.03220987319946s Eval Time: 15.569308996200562s\n",
    "# New minimum validation loss: 7.725741. Saving model ...\n",
    "# Epoch: 05, Train Loss: 7.784, Val Loss: 7.726 Train Time: 138.2921051979065s Eval Time: 15.532844543457031s\n",
    "# New minimum validation loss: 7.636809. Saving model ...\n",
    "# Epoch: 06, Train Loss: 7.686, Val Loss: 7.637 Train Time: 137.98424077033997s Eval Time: 15.647210597991943s\n",
    "# Epoch: 07, Train Loss: 7.605, Val Loss: 7.561 Train Time: 138.06058549880981s Eval Time: 15.66235065460205s\n",
    "# New minimum validation loss: 7.492020. Saving model ...\n",
    "# Epoch: 08, Train Loss: 7.532, Val Loss: 7.492 Train Time: 137.3553535938263s Eval Time: 15.439005374908447s\n",
    "# Epoch: 09, Train Loss: 7.467, Val Loss: 7.433 Train Time: 139.83897972106934s Eval Time: 15.809000015258789s\n",
    "# New minimum validation loss: 7.379917. Saving model ...\n",
    "# Epoch: 10, Train Loss: 7.412, Val Loss: 7.380 Train Time: 137.8189413547516s Eval Time: 15.560516834259033s\n",
    "# Epoch: 11, Train Loss: 7.360, Val Loss: 7.332 Train Time: 136.65642929077148s Eval Time: 15.613484621047974s\n",
    "# New minimum validation loss: 7.288950. Saving model ...\n",
    "# Epoch: 12, Train Loss: 7.313, Val Loss: 7.289 Train Time: 137.94713592529297s Eval Time: 15.612228393554688s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  sqlalchemy==1.4.8  optuna | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 23:08:48,921] A new study created in memory with name: no-name-1f73075e-d6d6-42e0-bb28-89e1c8bbfc17\n",
      "/tmp/ipykernel_537/4151047639.py:17: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  momentum = trial.suggest_uniform('momentum', 0.1, 0.9)\n",
      "/tmp/ipykernel_537/4151047639.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0:\tHyperparameters={'encoder_dropout': 0.10507486873471816, 'decoder_dropout': 0.40833098697311565, 'embedding_size': 240, 'hidden_size': 932, 'num_layers': 2, 'momentum': 0.41814445054158134, 'learning_rate': 0.00011683107152760196, 'weight_decay': 0.0037540186215223427, 'batch_size': 71}\n",
      "New minimum validation loss: 10.646195. Saving model ...\n",
      "Epoch: 01, Train Loss: 10.651, Val Loss: 10.646 Train Time: 237.41277265548706s Eval Time: 22.522602796554565s\n",
      "Epoch: 02, Train Loss: 10.635, Val Loss: 10.624 Train Time: 237.06071305274963s Eval Time: 22.41866135597229s\n",
      "Epoch: 03, Train Loss: 10.615, Val Loss: 10.600 Train Time: 235.52900218963623s Eval Time: 22.411000728607178s\n",
      "Epoch: 04, Train Loss: 10.595, Val Loss: 10.575 Train Time: 235.67363333702087s Eval Time: 22.463497400283813s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 23:30:33,565] Trial 0 finished with value: 10.646194859554893 and parameters: {'encoder_dropout': 0.10507486873471816, 'decoder_dropout': 0.40833098697311565, 'embedding_size': 240, 'hidden_size': 932, 'num_layers': 2, 'momentum': 0.41814445054158134, 'learning_rate': 0.00011683107152760196, 'weight_decay': 0.0037540186215223427, 'batch_size': 71}. Best is trial 0 with value: 10.646194859554893.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 10.574, Val Loss: 10.549 Train Time: 237.03408646583557s Eval Time: 22.42497444152832s\n",
      "Starting trial 1:\tHyperparameters={'encoder_dropout': 0.665348632640786, 'decoder_dropout': 0.687053155548927, 'embedding_size': 444, 'hidden_size': 782, 'num_layers': 2, 'momentum': 0.6949344175628585, 'learning_rate': 0.00899846356647276, 'weight_decay': 0.006562449189699982, 'batch_size': 790}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:30:35,325] Trial 1 failed with parameters: {'encoder_dropout': 0.665348632640786, 'decoder_dropout': 0.687053155548927, 'embedding_size': 444, 'hidden_size': 782, 'num_layers': 2, 'momentum': 0.6949344175628585, 'learning_rate': 0.00899846356647276, 'weight_decay': 0.006562449189699982, 'batch_size': 790} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.90 GiB (GPU 0; 22.20 GiB total capacity; 18.99 GiB already allocated; 141.12 MiB free; 20.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.90 GiB (GPU 0; 22.20 GiB total capacity; 18.99 GiB already allocated; 141.12 MiB free; 20.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:30:35,326] Trial 1 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 2:\tHyperparameters={'encoder_dropout': 0.12269443211360304, 'decoder_dropout': 0.27968200261748355, 'embedding_size': 415, 'hidden_size': 194, 'num_layers': 1, 'momentum': 0.6686117786661031, 'learning_rate': 0.009988780741110507, 'weight_decay': 0.0011716847959572971, 'batch_size': 990}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.12269443211360304 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.27968200261748355 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-18 23:30:37,368] Trial 2 failed with parameters: {'encoder_dropout': 0.12269443211360304, 'decoder_dropout': 0.27968200261748355, 'embedding_size': 415, 'hidden_size': 194, 'num_layers': 1, 'momentum': 0.6686117786661031, 'learning_rate': 0.009988780741110507, 'weight_decay': 0.0011716847959572971, 'batch_size': 990} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 22.20 GiB total capacity; 17.00 GiB already allocated; 1.20 GiB free; 19.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 22.20 GiB total capacity; 17.00 GiB already allocated; 1.20 GiB free; 19.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:30:37,369] Trial 2 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 3:\tHyperparameters={'encoder_dropout': 0.31464974385354144, 'decoder_dropout': 0.47639274120392805, 'embedding_size': 417, 'hidden_size': 617, 'num_layers': 2, 'momentum': 0.34083918942049596, 'learning_rate': 0.007282064530770312, 'weight_decay': 5.698672313824346e-06, 'batch_size': 672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:30:39,011] Trial 3 failed with parameters: {'encoder_dropout': 0.31464974385354144, 'decoder_dropout': 0.47639274120392805, 'embedding_size': 417, 'hidden_size': 617, 'num_layers': 2, 'momentum': 0.34083918942049596, 'learning_rate': 0.007282064530770312, 'weight_decay': 5.698672313824346e-06, 'batch_size': 672} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.09 GiB (GPU 0; 22.20 GiB total capacity; 15.39 GiB already allocated; 2.48 GiB free; 18.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 69, in objective\n",
      "    output = model(src, trg)        #  [trg length, batch size, output dim]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 97, in forward\n",
      "    _, hidden, cell = self.encoder(src)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 26, in forward\n",
      "    output, (hidden, cell) = self.lstm(embedded)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.09 GiB (GPU 0; 22.20 GiB total capacity; 15.39 GiB already allocated; 2.48 GiB free; 18.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:30:39,011] Trial 3 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 4:\tHyperparameters={'encoder_dropout': 0.5005741809297067, 'decoder_dropout': 0.3813415180682884, 'embedding_size': 341, 'hidden_size': 243, 'num_layers': 2, 'momentum': 0.772342884257848, 'learning_rate': 0.00015577481472992278, 'weight_decay': 0.0013004440297274059, 'batch_size': 112}\n",
      "New minimum validation loss: 10.611036. Saving model ...\n",
      "Epoch: 01, Train Loss: 10.641, Val Loss: 10.611 Train Time: 135.4051811695099s Eval Time: 15.971313238143921s\n",
      "Epoch: 02, Train Loss: 10.600, Val Loss: 10.562 Train Time: 134.778546333313s Eval Time: 15.887039184570312s\n",
      "Epoch: 03, Train Loss: 10.556, Val Loss: 10.507 Train Time: 134.14688801765442s Eval Time: 15.866497993469238s\n",
      "New minimum validation loss: 10.446149. Saving model ...\n",
      "Epoch: 04, Train Loss: 10.511, Val Loss: 10.446 Train Time: 134.47113823890686s Eval Time: 15.913512706756592s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-18 23:43:24,781] Trial 4 finished with value: 10.446149346175467 and parameters: {'encoder_dropout': 0.5005741809297067, 'decoder_dropout': 0.3813415180682884, 'embedding_size': 341, 'hidden_size': 243, 'num_layers': 2, 'momentum': 0.772342884257848, 'learning_rate': 0.00015577481472992278, 'weight_decay': 0.0013004440297274059, 'batch_size': 112}. Best is trial 4 with value: 10.446149346175467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 10.459, Val Loss: 10.376 Train Time: 135.02861952781677s Eval Time: 15.879432916641235s\n",
      "Starting trial 5:\tHyperparameters={'encoder_dropout': 0.2733795509951824, 'decoder_dropout': 0.343888067266337, 'embedding_size': 456, 'hidden_size': 508, 'num_layers': 2, 'momentum': 0.256622034391055, 'learning_rate': 0.003168780250203357, 'weight_decay': 0.0008225111358391471, 'batch_size': 310}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:43:27,447] Trial 5 failed with parameters: {'encoder_dropout': 0.2733795509951824, 'decoder_dropout': 0.343888067266337, 'embedding_size': 456, 'hidden_size': 508, 'num_layers': 2, 'momentum': 0.256622034391055, 'learning_rate': 0.003168780250203357, 'weight_decay': 0.0008225111358391471, 'batch_size': 310} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 22.20 GiB total capacity; 17.38 GiB already allocated; 1.31 GiB free; 19.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 22.20 GiB total capacity; 17.38 GiB already allocated; 1.31 GiB free; 19.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:27,448] Trial 5 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 6:\tHyperparameters={'encoder_dropout': 0.2853476218666638, 'decoder_dropout': 0.5584316370001761, 'embedding_size': 507, 'hidden_size': 786, 'num_layers': 2, 'momentum': 0.1355060402391354, 'learning_rate': 0.07311490005585702, 'weight_decay': 2.6235359269192113e-05, 'batch_size': 724}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:43:29,319] Trial 6 failed with parameters: {'encoder_dropout': 0.2853476218666638, 'decoder_dropout': 0.5584316370001761, 'embedding_size': 507, 'hidden_size': 786, 'num_layers': 2, 'momentum': 0.1355060402391354, 'learning_rate': 0.07311490005585702, 'weight_decay': 2.6235359269192113e-05, 'batch_size': 724} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 22.20 GiB total capacity; 18.17 GiB already allocated; 887.12 MiB free; 20.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 22.20 GiB total capacity; 18.17 GiB already allocated; 887.12 MiB free; 20.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:29,320] Trial 6 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 7:\tHyperparameters={'encoder_dropout': 0.5155779276085126, 'decoder_dropout': 0.4020495189941875, 'embedding_size': 512, 'hidden_size': 113, 'num_layers': 1, 'momentum': 0.7829295396101653, 'learning_rate': 0.00012648102378313243, 'weight_decay': 1.076582401579101e-06, 'batch_size': 666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5155779276085126 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4020495189941875 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-18 23:43:30,701] Trial 7 failed with parameters: {'encoder_dropout': 0.5155779276085126, 'decoder_dropout': 0.4020495189941875, 'embedding_size': 512, 'hidden_size': 113, 'num_layers': 1, 'momentum': 0.7829295396101653, 'learning_rate': 0.00012648102378313243, 'weight_decay': 1.076582401579101e-06, 'batch_size': 666} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.29 GiB (GPU 0; 22.20 GiB total capacity; 16.37 GiB already allocated; 863.12 MiB free; 20.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.29 GiB (GPU 0; 22.20 GiB total capacity; 16.37 GiB already allocated; 863.12 MiB free; 20.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:30,702] Trial 7 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 8:\tHyperparameters={'encoder_dropout': 0.5727601038025544, 'decoder_dropout': 0.47151524367295394, 'embedding_size': 502, 'hidden_size': 440, 'num_layers': 1, 'momentum': 0.8657430956780438, 'learning_rate': 0.010950242624732218, 'weight_decay': 0.0002677884860718867, 'batch_size': 613}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5727601038025544 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.47151524367295394 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-18 23:43:32,160] Trial 8 failed with parameters: {'encoder_dropout': 0.5727601038025544, 'decoder_dropout': 0.47151524367295394, 'embedding_size': 502, 'hidden_size': 440, 'num_layers': 1, 'momentum': 0.8657430956780438, 'learning_rate': 0.010950242624732218, 'weight_decay': 0.0002677884860718867, 'batch_size': 613} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.84 GiB (GPU 0; 22.20 GiB total capacity; 16.49 GiB already allocated; 889.12 MiB free; 20.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.84 GiB (GPU 0; 22.20 GiB total capacity; 16.49 GiB already allocated; 889.12 MiB free; 20.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:32,161] Trial 8 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 9:\tHyperparameters={'encoder_dropout': 0.3527430631179923, 'decoder_dropout': 0.3791869782445797, 'embedding_size': 331, 'hidden_size': 375, 'num_layers': 1, 'momentum': 0.6392604549916208, 'learning_rate': 0.09714692918736852, 'weight_decay': 0.0002236830893184973, 'batch_size': 508}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3527430631179923 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3791869782445797 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-18 23:43:33,300] Trial 9 failed with parameters: {'encoder_dropout': 0.3527430631179923, 'decoder_dropout': 0.3791869782445797, 'embedding_size': 331, 'hidden_size': 375, 'num_layers': 1, 'momentum': 0.6392604549916208, 'learning_rate': 0.09714692918736852, 'weight_decay': 0.0002236830893184973, 'batch_size': 508} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.35 GiB (GPU 0; 22.20 GiB total capacity; 14.99 GiB already allocated; 787.12 MiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.35 GiB (GPU 0; 22.20 GiB total capacity; 14.99 GiB already allocated; 787.12 MiB free; 20.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:33,301] Trial 9 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 10:\tHyperparameters={'encoder_dropout': 0.20699750452565896, 'decoder_dropout': 0.26934464593124813, 'embedding_size': 413, 'hidden_size': 895, 'num_layers': 3, 'momentum': 0.6443418821750319, 'learning_rate': 0.011054897577873276, 'weight_decay': 0.0005083733165769736, 'batch_size': 1004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:43:35,595] Trial 10 failed with parameters: {'encoder_dropout': 0.20699750452565896, 'decoder_dropout': 0.26934464593124813, 'embedding_size': 413, 'hidden_size': 895, 'num_layers': 3, 'momentum': 0.6443418821750319, 'learning_rate': 0.011054897577873276, 'weight_decay': 0.0005083733165769736, 'batch_size': 1004} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.86 GiB (GPU 0; 22.20 GiB total capacity; 15.56 GiB already allocated; 2.72 GiB free; 18.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 69, in objective\n",
      "    output = model(src, trg)        #  [trg length, batch size, output dim]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 97, in forward\n",
      "    _, hidden, cell = self.encoder(src)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 26, in forward\n",
      "    output, (hidden, cell) = self.lstm(embedded)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.86 GiB (GPU 0; 22.20 GiB total capacity; 15.56 GiB already allocated; 2.72 GiB free; 18.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:35,596] Trial 10 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 11:\tHyperparameters={'encoder_dropout': 0.34322977143308675, 'decoder_dropout': 0.39966871653854585, 'embedding_size': 371, 'hidden_size': 618, 'num_layers': 2, 'momentum': 0.377967845819174, 'learning_rate': 2.2494431094821257e-05, 'weight_decay': 0.0045260028142242335, 'batch_size': 705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-18 23:43:37,121] Trial 11 failed with parameters: {'encoder_dropout': 0.34322977143308675, 'decoder_dropout': 0.39966871653854585, 'embedding_size': 371, 'hidden_size': 618, 'num_layers': 2, 'momentum': 0.377967845819174, 'learning_rate': 2.2494431094821257e-05, 'weight_decay': 0.0045260028142242335, 'batch_size': 705} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 22.20 GiB total capacity; 16.17 GiB already allocated; 2.72 GiB free; 18.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 69, in objective\n",
      "    output = model(src, trg)        #  [trg length, batch size, output dim]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 97, in forward\n",
      "    _, hidden, cell = self.encoder(src)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 26, in forward\n",
      "    output, (hidden, cell) = self.lstm(embedded)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 22.20 GiB total capacity; 16.17 GiB already allocated; 2.72 GiB free; 18.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:37,122] Trial 11 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 12:\tHyperparameters={'encoder_dropout': 0.6603852326059189, 'decoder_dropout': 0.16986738272001356, 'embedding_size': 368, 'hidden_size': 697, 'num_layers': 1, 'momentum': 0.6667244061538032, 'learning_rate': 0.000381074516984359, 'weight_decay': 0.0012605640214337886, 'batch_size': 809}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6603852326059189 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16986738272001356 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-18 23:43:38,670] Trial 12 failed with parameters: {'encoder_dropout': 0.6603852326059189, 'decoder_dropout': 0.16986738272001356, 'embedding_size': 368, 'hidden_size': 697, 'num_layers': 1, 'momentum': 0.6667244061538032, 'learning_rate': 0.000381074516984359, 'weight_decay': 0.0012605640214337886, 'batch_size': 809} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 22.20 GiB total capacity; 15.95 GiB already allocated; 1.20 GiB free; 19.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 22.20 GiB total capacity; 15.95 GiB already allocated; 1.20 GiB free; 19.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-18 23:43:38,670] Trial 12 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 13:\tHyperparameters={'encoder_dropout': 0.22955902660914207, 'decoder_dropout': 0.4911158328281042, 'embedding_size': 491, 'hidden_size': 975, 'num_layers': 1, 'momentum': 0.6972683887927308, 'learning_rate': 0.0006676334684955292, 'weight_decay': 0.0033224254292016676, 'batch_size': 124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.22955902660914207 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4911158328281042 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum validation loss: 9.367463. Saving model ...\n",
      "Epoch: 01, Train Loss: 10.423, Val Loss: 9.367 Train Time: 188.44334268569946s Eval Time: 19.47628903388977s\n",
      "New minimum validation loss: 8.874996. Saving model ...\n",
      "Epoch: 02, Train Loss: 9.313, Val Loss: 8.875 Train Time: 187.65702390670776s Eval Time: 19.408082485198975s\n",
      "New minimum validation loss: 8.588095. Saving model ...\n",
      "Epoch: 03, Train Loss: 8.907, Val Loss: 8.588 Train Time: 189.47026109695435s Eval Time: 19.43692922592163s\n",
      "New minimum validation loss: 8.196831. Saving model ...\n",
      "Epoch: 04, Train Loss: 8.562, Val Loss: 8.197 Train Time: 186.49147987365723s Eval Time: 19.44693946838379s\n",
      "New minimum validation loss: 8.002150. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-19 00:01:53,886] Trial 13 finished with value: 8.002150303880933 and parameters: {'encoder_dropout': 0.22955902660914207, 'decoder_dropout': 0.4911158328281042, 'embedding_size': 491, 'hidden_size': 975, 'num_layers': 1, 'momentum': 0.6972683887927308, 'learning_rate': 0.0006676334684955292, 'weight_decay': 0.0033224254292016676, 'batch_size': 124}. Best is trial 13 with value: 8.002150303880933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 8.267, Val Loss: 8.002 Train Time: 188.1686635017395s Eval Time: 19.417216300964355s\n",
      "Starting trial 14:\tHyperparameters={'encoder_dropout': 0.1373715296914047, 'decoder_dropout': 0.26047182943288816, 'embedding_size': 395, 'hidden_size': 189, 'num_layers': 3, 'momentum': 0.8867896504624373, 'learning_rate': 2.7766072135128198e-05, 'weight_decay': 0.007428208680892792, 'batch_size': 306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:02:43,068] Trial 14 failed with parameters: {'encoder_dropout': 0.1373715296914047, 'decoder_dropout': 0.26047182943288816, 'embedding_size': 395, 'hidden_size': 189, 'num_layers': 3, 'momentum': 0.8867896504624373, 'learning_rate': 2.7766072135128198e-05, 'weight_decay': 0.007428208680892792, 'batch_size': 306} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 1.71 GiB (GPU 0; 22.20 GiB total capacity; 15.38 GiB already allocated; 765.12 MiB free; 20.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.71 GiB (GPU 0; 22.20 GiB total capacity; 15.38 GiB already allocated; 765.12 MiB free; 20.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:02:43,069] Trial 14 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 15:\tHyperparameters={'encoder_dropout': 0.2885671633365916, 'decoder_dropout': 0.5937232621311533, 'embedding_size': 362, 'hidden_size': 787, 'num_layers': 3, 'momentum': 0.7163704242654813, 'learning_rate': 0.09004233783138954, 'weight_decay': 0.00032880978500196144, 'batch_size': 348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:02:44,500] Trial 15 failed with parameters: {'encoder_dropout': 0.2885671633365916, 'decoder_dropout': 0.5937232621311533, 'embedding_size': 362, 'hidden_size': 787, 'num_layers': 3, 'momentum': 0.7163704242654813, 'learning_rate': 0.09004233783138954, 'weight_decay': 0.00032880978500196144, 'batch_size': 348} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 1.61 GiB (GPU 0; 22.20 GiB total capacity; 18.81 GiB already allocated; 263.12 MiB free; 20.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.61 GiB (GPU 0; 22.20 GiB total capacity; 18.81 GiB already allocated; 263.12 MiB free; 20.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:02:44,501] Trial 15 failed with value None.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5292350822860933 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 16:\tHyperparameters={'encoder_dropout': 0.5292350822860933, 'decoder_dropout': 0.6123409550559717, 'embedding_size': 220, 'hidden_size': 809, 'num_layers': 1, 'momentum': 0.7342863109426501, 'learning_rate': 0.0055993120385844, 'weight_decay': 1.0839056244755194e-06, 'batch_size': 721}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6123409550559717 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-19 00:02:46,240] Trial 16 failed with parameters: {'encoder_dropout': 0.5292350822860933, 'decoder_dropout': 0.6123409550559717, 'embedding_size': 220, 'hidden_size': 809, 'num_layers': 1, 'momentum': 0.7342863109426501, 'learning_rate': 0.0055993120385844, 'weight_decay': 1.0839056244755194e-06, 'batch_size': 721} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.68 GiB (GPU 0; 22.20 GiB total capacity; 15.77 GiB already allocated; 695.12 MiB free; 20.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.68 GiB (GPU 0; 22.20 GiB total capacity; 15.77 GiB already allocated; 695.12 MiB free; 20.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:02:46,241] Trial 16 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 17:\tHyperparameters={'encoder_dropout': 0.6702518122746408, 'decoder_dropout': 0.6019692277316478, 'embedding_size': 417, 'hidden_size': 158, 'num_layers': 2, 'momentum': 0.5093128260259621, 'learning_rate': 0.0027215101078588897, 'weight_decay': 9.805769151578178e-06, 'batch_size': 893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:02:48,473] Trial 17 failed with parameters: {'encoder_dropout': 0.6702518122746408, 'decoder_dropout': 0.6019692277316478, 'embedding_size': 417, 'hidden_size': 158, 'num_layers': 2, 'momentum': 0.5093128260259621, 'learning_rate': 0.0027215101078588897, 'weight_decay': 9.805769151578178e-06, 'batch_size': 893} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.85 GiB (GPU 0; 22.20 GiB total capacity; 12.77 GiB already allocated; 165.12 MiB free; 20.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.85 GiB (GPU 0; 22.20 GiB total capacity; 12.77 GiB already allocated; 165.12 MiB free; 20.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:02:48,474] Trial 17 failed with value None.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14014043002807133 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 18:\tHyperparameters={'encoder_dropout': 0.14014043002807133, 'decoder_dropout': 0.1674220731274399, 'embedding_size': 254, 'hidden_size': 744, 'num_layers': 1, 'momentum': 0.559313503017641, 'learning_rate': 0.037775260202830924, 'weight_decay': 0.0006143937477265973, 'batch_size': 850}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1674220731274399 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-19 00:02:50,068] Trial 18 failed with parameters: {'encoder_dropout': 0.14014043002807133, 'decoder_dropout': 0.1674220731274399, 'embedding_size': 254, 'hidden_size': 744, 'num_layers': 1, 'momentum': 0.559313503017641, 'learning_rate': 0.037775260202830924, 'weight_decay': 0.0006143937477265973, 'batch_size': 850} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.79 GiB (GPU 0; 22.20 GiB total capacity; 13.53 GiB already allocated; 161.12 MiB free; 20.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.79 GiB (GPU 0; 22.20 GiB total capacity; 13.53 GiB already allocated; 161.12 MiB free; 20.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:02:50,069] Trial 18 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 19:\tHyperparameters={'encoder_dropout': 0.18872017707563837, 'decoder_dropout': 0.26471499196465104, 'embedding_size': 345, 'hidden_size': 809, 'num_layers': 1, 'momentum': 0.8770181216883318, 'learning_rate': 0.0006037527750744888, 'weight_decay': 1.0067442886238327e-06, 'batch_size': 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.18872017707563837 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.26471499196465104 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum validation loss: 8.083847. Saving model ...\n",
      "Epoch: 01, Train Loss: 9.239, Val Loss: 8.084 Train Time: 202.3244276046753s Eval Time: 21.45066547393799s\n",
      "New minimum validation loss: 7.732253. Saving model ...\n",
      "Epoch: 02, Train Loss: 7.929, Val Loss: 7.732 Train Time: 202.05861020088196s Eval Time: 21.439218282699585s\n",
      "New minimum validation loss: 7.562543. Saving model ...\n",
      "Epoch: 03, Train Loss: 7.652, Val Loss: 7.563 Train Time: 203.92939352989197s Eval Time: 21.465781211853027s\n",
      "New minimum validation loss: 7.436394. Saving model ...\n",
      "Epoch: 04, Train Loss: 7.498, Val Loss: 7.436 Train Time: 201.30253672599792s Eval Time: 21.455437898635864s\n",
      "New minimum validation loss: 7.340739. Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-19 00:22:13,189] Trial 19 finished with value: 7.340739211429556 and parameters: {'encoder_dropout': 0.18872017707563837, 'decoder_dropout': 0.26471499196465104, 'embedding_size': 345, 'hidden_size': 809, 'num_layers': 1, 'momentum': 0.8770181216883318, 'learning_rate': 0.0006037527750744888, 'weight_decay': 1.0067442886238327e-06, 'batch_size': 62}. Best is trial 19 with value: 7.340739211429556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 7.384, Val Loss: 7.341 Train Time: 203.43801403045654s Eval Time: 21.4598548412323s\n",
      "Starting trial 20:\tHyperparameters={'encoder_dropout': 0.6184171647212484, 'decoder_dropout': 0.518001303217815, 'embedding_size': 298, 'hidden_size': 323, 'num_layers': 3, 'momentum': 0.8038799036735065, 'learning_rate': 7.247032415215429e-05, 'weight_decay': 3.5126210576018026e-06, 'batch_size': 617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:22:14,348] Trial 20 failed with parameters: {'encoder_dropout': 0.6184171647212484, 'decoder_dropout': 0.518001303217815, 'embedding_size': 298, 'hidden_size': 323, 'num_layers': 3, 'momentum': 0.8038799036735065, 'learning_rate': 7.247032415215429e-05, 'weight_decay': 3.5126210576018026e-06, 'batch_size': 617} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 22.20 GiB total capacity; 13.47 GiB already allocated; 201.12 MiB free; 20.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 22.20 GiB total capacity; 13.47 GiB already allocated; 201.12 MiB free; 20.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:22:14,349] Trial 20 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 21:\tHyperparameters={'encoder_dropout': 0.42191202885049683, 'decoder_dropout': 0.35222147502883105, 'embedding_size': 494, 'hidden_size': 216, 'num_layers': 1, 'momentum': 0.19295739923086677, 'learning_rate': 0.029475365616427017, 'weight_decay': 1.4298691564921317e-05, 'batch_size': 889}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.42191202885049683 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35222147502883105 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-19 00:22:16,336] Trial 21 failed with parameters: {'encoder_dropout': 0.42191202885049683, 'decoder_dropout': 0.35222147502883105, 'embedding_size': 494, 'hidden_size': 216, 'num_layers': 1, 'momentum': 0.19295739923086677, 'learning_rate': 0.029475365616427017, 'weight_decay': 1.4298691564921317e-05, 'batch_size': 889} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 4.96 GiB (GPU 0; 22.20 GiB total capacity; 9.93 GiB already allocated; 2.81 GiB free; 18.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 83, in objective\n",
      "    loss = criterion(output, trg)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.96 GiB (GPU 0; 22.20 GiB total capacity; 9.93 GiB already allocated; 2.81 GiB free; 18.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:22:16,337] Trial 21 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 22:\tHyperparameters={'encoder_dropout': 0.47739729381711005, 'decoder_dropout': 0.33786012859128356, 'embedding_size': 403, 'hidden_size': 267, 'num_layers': 3, 'momentum': 0.3100514768252849, 'learning_rate': 5.539001686622702e-05, 'weight_decay': 0.00046915331394621997, 'batch_size': 922}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:22:18,145] Trial 22 failed with parameters: {'encoder_dropout': 0.47739729381711005, 'decoder_dropout': 0.33786012859128356, 'embedding_size': 403, 'hidden_size': 267, 'num_layers': 3, 'momentum': 0.3100514768252849, 'learning_rate': 5.539001686622702e-05, 'weight_decay': 0.00046915331394621997, 'batch_size': 922} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 5.09 GiB (GPU 0; 22.20 GiB total capacity; 10.91 GiB already allocated; 2.86 GiB free; 18.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 69, in objective\n",
      "    output = model(src, trg)        #  [trg length, batch size, output dim]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 97, in forward\n",
      "    _, hidden, cell = self.encoder(src)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 26, in forward\n",
      "    output, (hidden, cell) = self.lstm(embedded)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.09 GiB (GPU 0; 22.20 GiB total capacity; 10.91 GiB already allocated; 2.86 GiB free; 18.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:22:18,146] Trial 22 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 23:\tHyperparameters={'encoder_dropout': 0.5880826173004987, 'decoder_dropout': 0.6052703992184986, 'embedding_size': 466, 'hidden_size': 1003, 'num_layers': 1, 'momentum': 0.29204042239105077, 'learning_rate': 0.00010953862251267148, 'weight_decay': 2.3167073523288925e-06, 'batch_size': 521}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5880826173004987 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6052703992184986 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[W 2023-11-19 00:22:22,113] Trial 23 failed with parameters: {'encoder_dropout': 0.5880826173004987, 'decoder_dropout': 0.6052703992184986, 'embedding_size': 466, 'hidden_size': 1003, 'num_layers': 1, 'momentum': 0.29204042239105077, 'learning_rate': 0.00010953862251267148, 'weight_decay': 2.3167073523288925e-06, 'batch_size': 521} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.49 GiB (GPU 0; 22.20 GiB total capacity; 13.72 GiB already allocated; 353.12 MiB free; 20.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.49 GiB (GPU 0; 22.20 GiB total capacity; 13.72 GiB already allocated; 353.12 MiB free; 20.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:22:22,114] Trial 23 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 24:\tHyperparameters={'encoder_dropout': 0.3010150382388159, 'decoder_dropout': 0.5424255469565925, 'embedding_size': 414, 'hidden_size': 786, 'num_layers': 3, 'momentum': 0.10674102239085653, 'learning_rate': 0.022079003980209552, 'weight_decay': 9.497198441200903e-06, 'batch_size': 457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:22:27,495] Trial 24 failed with parameters: {'encoder_dropout': 0.3010150382388159, 'decoder_dropout': 0.5424255469565925, 'embedding_size': 414, 'hidden_size': 786, 'num_layers': 3, 'momentum': 0.10674102239085653, 'learning_rate': 0.022079003980209552, 'weight_decay': 9.497198441200903e-06, 'batch_size': 457} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.17 GiB (GPU 0; 22.20 GiB total capacity; 10.32 GiB already allocated; 2.86 GiB free; 18.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 69, in objective\n",
      "    output = model(src, trg)        #  [trg length, batch size, output dim]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 97, in forward\n",
      "    _, hidden, cell = self.encoder(src)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_537/2566751811.py\", line 26, in forward\n",
      "    output, (hidden, cell) = self.lstm(embedded)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 812, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.17 GiB (GPU 0; 22.20 GiB total capacity; 10.32 GiB already allocated; 2.86 GiB free; 18.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:22:27,496] Trial 24 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 25:\tHyperparameters={'encoder_dropout': 0.17476255006006935, 'decoder_dropout': 0.18291147572145944, 'embedding_size': 474, 'hidden_size': 665, 'num_layers': 2, 'momentum': 0.5750458745402064, 'learning_rate': 2.0038030099816665e-05, 'weight_decay': 4.204537887096255e-05, 'batch_size': 301}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-11-19 00:24:13,908] Trial 25 failed with parameters: {'encoder_dropout': 0.17476255006006935, 'decoder_dropout': 0.18291147572145944, 'embedding_size': 474, 'hidden_size': 665, 'num_layers': 2, 'momentum': 0.5750458745402064, 'learning_rate': 2.0038030099816665e-05, 'weight_decay': 4.204537887096255e-05, 'batch_size': 301} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.26 GiB (GPU 0; 22.20 GiB total capacity; 12.98 GiB already allocated; 349.12 MiB free; 20.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_537/4151047639.py\", line 85, in objective\n",
      "    loss.backward()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.26 GiB (GPU 0; 22.20 GiB total capacity; 12.98 GiB already allocated; 349.12 MiB free; 20.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[W 2023-11-19 00:24:13,909] Trial 25 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 26:\tHyperparameters={'encoder_dropout': 0.666860866556084, 'decoder_dropout': 0.14310546756612483, 'embedding_size': 403, 'hidden_size': 686, 'num_layers': 1, 'momentum': 0.5304037074535177, 'learning_rate': 0.016803731754976535, 'weight_decay': 0.007561913049800378, 'batch_size': 62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.666860866556084 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14310546756612483 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum validation loss: 7.281041. Saving model ...\n",
      "Epoch: 01, Train Loss: 7.809, Val Loss: 7.281 Train Time: 190.14013409614563s Eval Time: 19.358969926834106s\n",
      "New minimum validation loss: 7.045174. Saving model ...\n",
      "Epoch: 02, Train Loss: 7.126, Val Loss: 7.045 Train Time: 188.74524021148682s Eval Time: 19.301857709884644s\n",
      "Epoch: 03, Train Loss: 6.984, Val Loss: 6.979 Train Time: 189.23022866249084s Eval Time: 19.335997819900513s\n",
      "New minimum validation loss: 6.916367. Saving model ...\n",
      "Epoch: 04, Train Loss: 6.911, Val Loss: 6.916 Train Time: 190.07164096832275s Eval Time: 19.33453106880188s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-19 00:42:04,122] Trial 26 finished with value: 6.916367074204839 and parameters: {'encoder_dropout': 0.666860866556084, 'decoder_dropout': 0.14310546756612483, 'embedding_size': 403, 'hidden_size': 686, 'num_layers': 1, 'momentum': 0.5304037074535177, 'learning_rate': 0.016803731754976535, 'weight_decay': 0.007561913049800378, 'batch_size': 62}. Best is trial 26 with value: 6.916367074204839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 6.869, Val Loss: 6.885 Train Time: 188.61841201782227s Eval Time: 19.39446473121643s\n",
      "Starting trial 27:\tHyperparameters={'encoder_dropout': 0.21880156466541706, 'decoder_dropout': 0.36376582554937464, 'embedding_size': 280, 'hidden_size': 960, 'num_layers': 1, 'momentum': 0.36099383777669003, 'learning_rate': 0.00032189008410052556, 'weight_decay': 4.067770746713994e-06, 'batch_size': 242}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.21880156466541706 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.36376582554937464 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New minimum validation loss: 10.654181. Saving model ...\n",
      "Epoch: 01, Train Loss: 10.660, Val Loss: 10.654 Train Time: 171.24145770072937s Eval Time: 19.288434982299805s\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import orm\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "encoder_input_size = len(question_context_field.vocab) \n",
    "decoder_output_size = len(answer_field.vocab)\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    encoder_dropout = trial.suggest_float('encoder_dropout', 0.1, 0.7)\n",
    "    decoder_dropout = trial.suggest_float('decoder_dropout', 0.1, 0.7)\n",
    "    embedding_size = trial.suggest_int('embedding_size', 200, 512)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 64, 1024)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    momentum = trial.suggest_uniform('momentum', 0.1, 0.9)\n",
    "        \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "    # opt = trial.suggest_categorical('opt', ['sgd', 'adam'])\n",
    "    # momentum = trial.suggest_uniform('momentum', 0.1, 0.9)\n",
    "    \n",
    "    batch_size = trial.suggest_int('batch_size', 32, 1024)\n",
    "    \n",
    "    print(f'Starting trial {trial.number}:\\tHyperparameters={trial.params}') \n",
    "    \n",
    "    train_iterator, valid_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "       batch_size=batch_size,\n",
    "       sort_within_batch=True,\n",
    "        sort_key = lambda x: len(x.question_context),\n",
    "        device=device)\n",
    "    \n",
    "    encoder = Encoder(encoder_input_size, hidden_size, embedding_size, num_layers, encoder_dropout)\n",
    "    decoder = Decoder(hidden_size, decoder_output_size, embedding_size, num_layers, decoder_dropout)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)    \n",
    "    \n",
    "    # Init model weights\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # https://pytorch.org/docs/2.0/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, threshold=0.01) # YOUR CODE HERE\n",
    "    \n",
    "    num_epochs = 5\n",
    "    clip = 5\n",
    "    valid_loss_min = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        avg_train_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            src = batch.question_context.to(device)\n",
    "            trg = batch.answer.to(device)\n",
    "\n",
    "    #         print('src:', src.shape)\n",
    "    #         print('trg:', trg.shape)        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # print (src.shape)\n",
    "            # print (trg.shape)\n",
    "\n",
    "            # Pass the source sequences through the encoder\n",
    "            output = model(src, trg)        #  [trg length, batch size, output dim]\n",
    "            # print('output:', output.shape)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "    #         print('output 2:', output.shape)\n",
    "    #         print('trg 2:', trg.shape)\n",
    "\n",
    "    #         print('output content:', output)\n",
    "    #         print('trg content:', trg)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "        average_train_loss = train_loss / len(train_iterator)\n",
    "\n",
    "        end_time = time.time()    \n",
    "        train_elapsed_time = end_time - start_time\n",
    "\n",
    "        # print(f'Epoch: {epoch+1:02} | Train Time: {train_elapsed_time}s')\n",
    "\n",
    "        # Evaluation on the test dataset\n",
    "        start_time = time.time()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                src = batch.question_context.to(device)\n",
    "                trg = batch.answer.to(device)                    \n",
    "\n",
    "                output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "                #trg = [trg len, batch size]\n",
    "                #output = [trg len, batch size, output dim]\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = trg[1:].view(-1)\n",
    "\n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            average_val_loss = valid_loss / len(valid_iterator)\n",
    "\n",
    "        end_time = time.time()    \n",
    "        val_elapsed_time = end_time - start_time\n",
    "\n",
    "        # print(f'Eval Time: {elapsed_time}')\n",
    "        # print(f'Epoch: {epoch+1:02} | Eval Time: {val_elapsed_time}s')\n",
    "\n",
    "        scheduler.step(average_val_loss)\n",
    "        \n",
    "        if valid_loss_min is None or (\n",
    "                (valid_loss_min - average_val_loss) / valid_loss_min > 0.01\n",
    "        ):\n",
    "            print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': train_loss,\n",
    "                }, f'checkpoints/hpo_{trial.number}_model.pt\",')\n",
    "\n",
    "            valid_loss_min = average_val_loss\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:02}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f} Train Time: {train_elapsed_time}s Eval Time: {val_elapsed_time}s\")    \n",
    "        \n",
    "    return valid_loss_min\n",
    "    \n",
    "# Define study and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(objective, n_trials=100, catch=(torch.cuda.OutOfMemoryError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: FrozenTrial(number=31, state=TrialState.COMPLETE, values=[6.840339520820101], datetime_start=datetime.datetime(2023, 11, 19, 1, 42, 47, 67924), datetime_complete=datetime.datetime(2023, 11, 19, 2, 0, 58, 397423), params={'encoder_dropout': 0.6039111450304973, 'decoder_dropout': 0.33154046338693915, 'embedding_size': 468, 'hidden_size': 938, 'num_layers': 1, 'momentum': 0.5494925017261035, 'learning_rate': 0.04835390488901119, 'weight_decay': 0.00014738274625279338, 'batch_size': 132}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'encoder_dropout': FloatDistribution(high=0.7, log=False, low=0.1, step=None), 'decoder_dropout': FloatDistribution(high=0.7, log=False, low=0.1, step=None), 'embedding_size': IntDistribution(high=512, log=False, low=200, step=1), 'hidden_size': IntDistribution(high=1024, log=False, low=64, step=1), 'num_layers': IntDistribution(high=3, log=False, low=1, step=1), 'momentum': FloatDistribution(high=0.9, log=False, low=0.1, step=None), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-06, step=None), 'batch_size': IntDistribution(high=1024, log=False, low=32, step=1)}, trial_id=31, value=None)\n",
      "Best hyperparameters: {'encoder_dropout': 0.6039111450304973, 'decoder_dropout': 0.33154046338693915, 'embedding_size': 468, 'hidden_size': 938, 'num_layers': 1, 'momentum': 0.5494925017261035, 'learning_rate': 0.04835390488901119, 'weight_decay': 0.00014738274625279338, 'batch_size': 132}\n",
      "Validation loss: 6.840339520820101\n"
     ]
    }
   ],
   "source": [
    "print('Best trial:', study.best_trial)\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Validation loss:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [I 2023-11-19 00:01:53,886] Trial 13 finished with value: 8.002150303880933 and parameters: {'encoder_dropout': 0.22955902660914207, 'decoder_dropout': 0.4911158328281042, 'embedding_size': 491, 'hidden_size': 975, 'num_layers': 1, 'momentum': 0.6972683887927308, 'learning_rate': 0.0006676334684955292, 'weight_decay': 0.0033224254292016676, 'batch_size': 124}. Best is trial 13 with value: 8.002150303880933.\n",
    "\n",
    "\n",
    "# [I 2023-11-19 00:22:13,189] Trial 19 finished with value: 7.340739211429556 and parameters: {'encoder_dropout': 0.18872017707563837, 'decoder_dropout': 0.26471499196465104, 'embedding_size': 345, 'hidden_size': 809, 'num_layers': 1, 'momentum': 0.8770181216883318, 'learning_rate': 0.0006037527750744888, 'weight_decay': 1.0067442886238327e-06, 'batch_size': 62}. Best is trial 19 with value: 7.340739211429556.\n",
    "\n",
    "# [I 2023-11-19 00:42:04,122] Trial 26 finished with value: 6.916367074204839 and parameters: {'encoder_dropout': 0.666860866556084, 'decoder_dropout': 0.14310546756612483, 'embedding_size': 403, 'hidden_size': 686, 'num_layers': 1, 'momentum': 0.5304037074535177, 'learning_rate': 0.016803731754976535, 'weight_decay': 0.007561913049800378, 'batch_size': 62}. Best is trial 26 with value: 6.916367074204839.\n",
    "\n",
    "\n",
    "# Best hyperparameters: {'encoder_dropout': 0.6039111450304973, 'decoder_dropout': 0.33154046338693915, 'embedding_size': 468, 'hidden_size': 938, 'num_layers': 1, 'momentum': 0.5494925017261035, 'learning_rate': 0.04835390488901119, 'weight_decay': 0.00014738274625279338, 'batch_size': 132}\n",
    "# Validation loss: 6.840339520820101\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "test_decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "inference_model = Seq2SeqInference(test_encoder, test_decoder, answer_field)\n",
    "inference_model.to(device)\n",
    "\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "test_criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # https://pytorch.org/docs/2.0/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "test_criterion.to(device)\n",
    "\n",
    "# checkpoint = torch.load('checkpoints/best_val_loss.pt')\n",
    "checkpoint = torch.load('checkpoints/train-25.pt')\n",
    "inference_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "test_iterator = BucketIterator(\n",
    "        # dataset=valid_data,\n",
    "        dataset=train_data,\n",
    "       batch_size=1,\n",
    "       sort_within_batch=True,\n",
    "        sort_key = lambda x: len(x.question_context),\n",
    "        device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "# Smoke text\n",
    "\n",
    "with torch.no_grad():\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "        \n",
    "        # print(\"src shape\", src.shape)\n",
    "        # print(\"src\", src)\n",
    "        # print(\"trg\", trg)\n",
    "        \n",
    "        tokens, logits = inference_model(src, 5)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #logits = [max length, batch, output dim]\n",
    "\n",
    "#         print(\"logits shape\", logits.shape)\n",
    "#         print(\"seq2seq-logits-raw\", logits)\n",
    "        \n",
    "#         logits_dim = logits.shape[-1]\n",
    "#         logits = logits[1:].view(-1, logits_dim)\n",
    "        \n",
    "        # trg = trg[1:].view(-1)\n",
    "\n",
    "        # print(\"seq2seq-logits\", logits)\n",
    "        # print(\"seq2seq-trg\", trg)\n",
    "        # print(\"tokens shape\", tokens.shape)\n",
    "        # print(\"tokens\", tokens)\n",
    "                \n",
    "        # response_text = tokens_to_string(tokens.squeeze().tolist())\n",
    "        response_text = decode_answer(tokens.squeeze().tolist())\n",
    "        \n",
    "        print(response_text)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        # TODO: question: should 'output' really be the MLP result tensor ?\n",
    "#         loss = test_criterion(output, trg)\n",
    "\n",
    "#         valid_loss += loss.item()\n",
    "        \n",
    "        break # TODO: remove line\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def respond_question(question):\n",
    "    with torch.no_grad():\n",
    "        question_tensor = encode_question(question)\n",
    "\n",
    "        tokens, logits = inference_model(src, 30)\n",
    "\n",
    "        response_text = decode_answer(tokens.squeeze().tolist())\n",
    "\n",
    "        return response_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond_question(\"A week comprise seven days. What day do you prefer ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respond_question(\"What would change the rotational inertia of a body under Newton's First Law of Motion? Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_question(\"What would change the rotational inertia of a body under Newton's First Law of Motion? Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_question(\"What changes macroscopic closed system energies? The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
