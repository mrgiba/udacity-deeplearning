{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Steps Overview and Estimated Duration\n",
    "# Below you will find each of the components of the project, and estimated times to complete each portion. \n",
    "# These are estimates and not exact timings to help you expect the amount of time necessary to put aside to work on your project.\n",
    "\n",
    "# Prepare data (~2 hours)\n",
    "# Build your vocabulary from a corpus of language data. The Vocabulary object is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Build Model (~4 hours)\n",
    "# Build your Encoder, Decoder, and larger Sequence to Sequence pattern in PyTorch. This pattern is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Train Model (~3 hours)\n",
    "# Write your training procedure and divide your dataset into train/test/validation splits. Then, train your network and plot your evaluation metrics. Save your model after it reaches a satisfactory level of accuracy.\n",
    "\n",
    "# Evaluate & Interact w/ Model (~1 hour)\n",
    "# Write a script to interact with your network at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Summary\n",
    "# The LSTM Chatbot will help you show off your skills as a deep learning practitioner. You will develop the chatbot using a new architecture called a Seq2Seq. \n",
    "# Additionally, you can use pre-trained word embeddings to improve the performance of your model. Let's get started by following the steps below:\n",
    "\n",
    "# Step 1: Build your Vocabulary & create the Word Embeddings\n",
    "# The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Use Gensim to extract the word embeddings from one of its corpus'.\n",
    "# Use NLTK and Gensim to create a function to clean your text and look up the index of a word's embeddings.\n",
    "\n",
    "# Step 2: Create the Encoder\n",
    "# A Seq2Seq architecture consists of an encoder and a decoder unit. You will use Pytorch to build a full Seq2Seq model.\n",
    "# The first step of the architecture is to create an encoder with an LSTM unit.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Load your pretrained embeddings into the LSTM unit.\n",
    "\n",
    "# Step 3: Create the Decoder\n",
    "# The second step of the architecture is to create a decoder using a second LSTM unit.\n",
    "\n",
    "# Step 4: Combine them into a Seq2Seq Architecture\n",
    "# To finalize your model, you will combine the encoder and decoder units into a working model.\n",
    "# The Seq2Seq2 model must be able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function.\n",
    "\n",
    "# Step 5: Train & evaluate your model\n",
    "# Finally you will train and evaluate your model using a Pytorch training loop.\n",
    "\n",
    "# Step 6: Interact with the Chatbot\n",
    "# Demonstrate your chatbot by converting the outputs of the model to text and displaying it's responses at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pre-requisite: Select PyTorch 2.00 kernel\n",
    "\n",
    "# Install requirements\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1 torchtext torchdata portalocker | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.15.1 portalocker>=2.0.0 | grep -v \"already satisfied\"\n",
    "!pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.6.0   | grep -v \"already satisfied\"\n",
    "\n",
    "#  torchtext==0.12.0 --> torch==1.11.0\n",
    "#  torchtext==0.13.0 --> torch==1.12.0\n",
    "#  torchtext==0.14.0 --> torch==1.12.0\n",
    "# torchtext==0.15.1 --> torch==2.0.0\n",
    "# torchtext==0.15.2 --> torch==2.0.1\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.10.0 | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.2.0 nltk torchtext  | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import sklearn.model_selection \n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "question_context_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "answer_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "def loadDF(path):    \n",
    "    data_file_path = download_from_url(path, root=\"data\")\n",
    "    \n",
    "    with open(data_file_path, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    examples = []\n",
    "    for item in squad_data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']                \n",
    "                # question_context = f\"{question} {separator_token} {context}\"\n",
    "                question_context = f\"{question} {context}\"\n",
    "                answer = qa['answers'][0]['text']\n",
    "                \n",
    "                data.append((question_context, answer))                                \n",
    "\n",
    "            #TODO: remove line below\n",
    "            # break    \n",
    "                \n",
    "    df = pd.DataFrame.from_records(data, columns=['question_context', 'answer'])        \n",
    "    df['question_context'] = df['question_context'].apply(prepare_text)\n",
    "    df['answer'] = df['answer'].apply(prepare_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    return sentence\n",
    "\n",
    "# def prepare_dataset(df):\n",
    "#     df['context_tokens'] = df['context'].apply(prepare_text)\n",
    "#     df['question_tokens'] = df['question'].apply(prepare_text)\n",
    "#     df['answer_tokens'] = df['answer'].apply(prepare_text)\n",
    "    \n",
    "#     df['context_ids'] = df['context_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['question_ids'] = df['question_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['answer_ids'] = df['answer_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "\n",
    "#     df['full_question_ids'] = df.apply(lambda row: [CONTEXT_index] + row['context_ids'] + [QUESTION_index] + row['question_ids'], axis=1)    \n",
    "\n",
    "# def train_test_split(SRC, TRG, test_size=0.2, random_seed=42):\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "    \n",
    "#     SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset= train_test_split(SRC, TRG, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "#     # Return the training and test datasets\n",
    "#     return SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset\n",
    "\n",
    "\n",
    "def load_datasets(df, random_seed=42):\n",
    "    df.to_csv(\"data/data.csv\", index=False)\n",
    "        \n",
    "    fields = [('question_context', question_context_field), ('answer', answer_field)]\n",
    "    dataset = TabularDataset(\"data/data.csv\", format='csv', fields=fields)\n",
    "    \n",
    "    question_context_field.build_vocab(dataset, min_freq=1)\n",
    "    answer_field.build_vocab(dataset, min_freq=1)\n",
    "\n",
    "    train_data, valid_data = dataset.split(split_ratio=0.8, random_state=random.seed(random_seed))\n",
    "    \n",
    "    return train_data, valid_data\n",
    "\n",
    "# def train_test_split(SRC, TRG,  test_size=0.2, random_seed=42):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102396\n",
      "42794\n"
     ]
    }
   ],
   "source": [
    "df = loadDF(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "\n",
    "train_data, valid_data = load_datasets(df)\n",
    "\n",
    "print(len(question_context_field.vocab))\n",
    "\n",
    "print(len(answer_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Buildi...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame? Architectura...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>What was Yangon previously known as? Kathmandu...</td>\n",
       "      <td>Rangoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>What is KMC an initialism of? Kathmandu Metrop...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_context   \n",
       "0      To whom did the Virgin Mary allegedly appear i...  \\\n",
       "1      What is in front of the Notre Dame Main Buildi...   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3      What is the Grotto at Notre Dame? Architectura...   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595  What was Yangon previously known as? Kathmandu...   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598  What is KMC an initialism of? Kathmandu Metrop...   \n",
       "\n",
       "                                        answer  \n",
       "0                   Saint Bernadette Soubirous  \n",
       "1                    a copper statue of Christ  \n",
       "2                            the Main Building  \n",
       "3      a Marian place of prayer and reflection  \n",
       "4           a golden statue of the Virgin Mary  \n",
       "...                                        ...  \n",
       "87594                                   Oregon  \n",
       "87595                                  Rangoon  \n",
       "87596                                    Minsk  \n",
       "87597                                     1975  \n",
       "87598              Kathmandu Metropolitan City  \n",
       "\n",
       "[87599 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where can a temple of the Jain faith be found? Sikhism is practiced primarily in Gurudwara at Kupundole. An earlier temple of Sikhism is also present in Kathmandu which is now defunct. Jainism is practiced by a small community. A Jain temple is present in Gyaneshwar, where Jains practice their faith. According to the records of the Spiritual Assembly of the Baha'is of Nepal, there are approximately 300 Baha'is in Kathmandu valley. They have a National Office located in Shantinagar, Baneshwor. The Baha'is also have classes for children at the National Centre and other localities in Kathmandu. Islam is practised in Kathmandu but Muslims are a minority, accounting for about 4.2% of the population of Nepal.[citation needed] It is said that in Kathmandu alone there are 170 Christian churches. Christian missionary hospitals, welfare organizations, and schools are also operating. Nepali citizens who served as soldiers in Indian and British armies, who had converted to Christianity while in service, on return to Nepal continue to practice their religion. They have contributed to the spread of Christianity and the building of churches in Nepal and in Kathmandu, in particular.\n",
      "Gyaneshwar\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[-30]['question_context'])\n",
    "\n",
    "print(df.iloc[-30]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oQLTP2Wmi1eB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)        # How to use it ????\n",
    "        # self.embedding_dim = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        # self.embedding = nn.Embedding(self.input_size, self.embedding_dim)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        embedded = self.embedding(i)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        return output, hidden, cell\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        # self.embedding = nn.Embedding(self.hidden_size, self.hidden_size)  # From lesson\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.embedding = nn.Embedding(hidden_size, embedding_size) # Why ?\n",
    "        \n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # print('[Decoder] input shape:', input.shape)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell)) \n",
    "        \n",
    "        \n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):              \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "                \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        input = trg[0, :]  \n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # get highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            input = trg[t] if use_teacher_forcing else top1\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the model and other parameters\n",
    "\n",
    "encoder_input_size = len(question_context_field.vocab) # vocabulary.get_size()\n",
    "encoder_embedding_size= 300\n",
    "# encoder_num_layers = 1\n",
    "encoder_dropout = 0.5\n",
    "\n",
    "decoder_output_size = len(answer_field.vocab) # vocabulary.get_size()\n",
    "# decoder_hidden_size = encoder_hidden_size\n",
    "decoder_embedding_size = 300\n",
    "# decoder_num_layers = 1\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "\n",
    "#     def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "   batch_size=batch_size,\n",
    "   sort_within_batch=True,\n",
    "    sort_key = lambda x: len(x.question_context),\n",
    "    device=device)\n",
    "                             \n",
    "# train_dataset = TensorDataset(src_train_tensor, trg_train_tensor)\n",
    "# val_dataset  = TensorDataset(src_val_tensor, trg_val_tensor)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Time: 177.93499732017517s\n",
      "Epoch: 01 | Eval Time: 22.525410413742065s\n",
      "New minimum validation loss: 6.887178. Saving model ...\n",
      "Epoch: 01, Train Loss: 6.981, Val Loss: 6.887\n",
      "Epoch: 02 | Train Time: 173.65457701683044s\n",
      "Epoch: 02 | Eval Time: 21.290350198745728s\n",
      "New minimum validation loss: 6.986544. Saving model ...\n",
      "Epoch: 02, Train Loss: 6.581, Val Loss: 6.987\n",
      "Epoch: 03 | Train Time: 172.9591293334961s\n",
      "Epoch: 03 | Eval Time: 21.31435537338257s\n",
      "New minimum validation loss: 7.041733. Saving model ...\n",
      "Epoch: 03, Train Loss: 6.466, Val Loss: 7.042\n",
      "Epoch: 04 | Train Time: 171.75205731391907s\n",
      "Epoch: 04 | Eval Time: 21.43007493019104s\n",
      "New minimum validation loss: 7.077194. Saving model ...\n",
      "Epoch: 04, Train Loss: 6.376, Val Loss: 7.077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "learning_rate = 0.001\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs =  20\n",
    "clip = 1\n",
    "valid_loss_min = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    avg_train_loss = 0.0\n",
    "    # epoch_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "    \n",
    "#         print('src:', src.shape)\n",
    "#         print('trg:', trg.shape)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # src = src.unsqueeze(1)\n",
    "        # trg = trg.unsqueeze(1)\n",
    "        \n",
    "        # print (src.shape)\n",
    "        # print (trg.shape)\n",
    "        \n",
    "        # Pass the source sequences through the encoder\n",
    "        output = model(src, trg) # output: [trg length, batch size, output dim]\n",
    "        # print('output:', output.shape)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "#         print('output 2:', output.shape)\n",
    "#         print('trg 2:', trg.shape)\n",
    "        \n",
    "#         print('output content:', output)\n",
    "#         print('trg content:', trg)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(train_loss)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # break\n",
    "        \n",
    "    average_train_loss = train_loss / len(train_iterator)\n",
    "    \n",
    "    end_time = time.time()    \n",
    "    train_elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Time: {train_elapsed_time}s')\n",
    "        \n",
    "          \n",
    "    # Evaluation on the test dataset\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            src = batch.question_context.to(device)\n",
    "            trg = batch.answer.to(device)                    \n",
    "            \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "        average_val_loss = valid_loss / len(valid_iterator)\n",
    "                    \n",
    "    end_time = time.time()    \n",
    "    val_elapsed_time = end_time - start_time\n",
    "    \n",
    "    # print(f'Eval Time: {elapsed_time}')\n",
    "    print(f'Epoch: {epoch+1:02} | Eval Time: {val_elapsed_time}s')\n",
    "    \n",
    "    if valid_loss_min is None or (\n",
    "            (valid_loss_min - average_val_loss) / valid_loss_min > 0.01\n",
    "    ):\n",
    "        print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, 'checkpoints/best_val_loss.pt')\n",
    "\n",
    "        valid_loss_min = average_val_loss\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1:02}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2],\n",
      "        [1818],\n",
      "        [ 124],\n",
      "        [   6],\n",
      "        [1289],\n",
      "        [   3]], device='cuda:0')\n",
      "mexico\n"
     ]
    }
   ],
   "source": [
    "# print(batch.question_context)\n",
    "\n",
    "print(batch.answer)\n",
    "\n",
    "print(answer_field.vocab.itos[980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
