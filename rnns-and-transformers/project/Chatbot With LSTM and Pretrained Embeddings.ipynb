{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Steps Overview and Estimated Duration\n",
    "# Below you will find each of the components of the project, and estimated times to complete each portion. \n",
    "# These are estimates and not exact timings to help you expect the amount of time necessary to put aside to work on your project.\n",
    "\n",
    "# Prepare data (~2 hours)\n",
    "# Build your vocabulary from a corpus of language data. The Vocabulary object is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Build Model (~4 hours)\n",
    "# Build your Encoder, Decoder, and larger Sequence to Sequence pattern in PyTorch. This pattern is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Train Model (~3 hours)\n",
    "# Write your training procedure and divide your dataset into train/test/validation splits. Then, train your network and plot your evaluation metrics. Save your model after it reaches a satisfactory level of accuracy.\n",
    "\n",
    "# Evaluate & Interact w/ Model (~1 hour)\n",
    "# Write a script to interact with your network at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Summary\n",
    "# The LSTM Chatbot will help you show off your skills as a deep learning practitioner. You will develop the chatbot using a new architecture called a Seq2Seq. \n",
    "# Additionally, you can use pre-trained word embeddings to improve the performance of your model. Let's get started by following the steps below:\n",
    "\n",
    "# Step 1: Build your Vocabulary & create the Word Embeddings\n",
    "# The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Use Gensim to extract the word embeddings from one of its corpus'.\n",
    "# Use NLTK and Gensim to create a function to clean your text and look up the index of a word's embeddings.\n",
    "\n",
    "# Step 2: Create the Encoder\n",
    "# A Seq2Seq architecture consists of an encoder and a decoder unit. You will use Pytorch to build a full Seq2Seq model.\n",
    "# The first step of the architecture is to create an encoder with an LSTM unit.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Load your pretrained embeddings into the LSTM unit.\n",
    "\n",
    "# Step 3: Create the Decoder\n",
    "# The second step of the architecture is to create a decoder using a second LSTM unit.\n",
    "\n",
    "# Step 4: Combine them into a Seq2Seq Architecture\n",
    "# To finalize your model, you will combine the encoder and decoder units into a working model.\n",
    "# The Seq2Seq2 model must be able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function.\n",
    "\n",
    "# Step 5: Train & evaluate your model\n",
    "# Finally you will train and evaluate your model using a Pytorch training loop.\n",
    "\n",
    "# Step 6: Interact with the Chatbot\n",
    "# Demonstrate your chatbot by converting the outputs of the model to text and displaying it's responses at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.3.1\n",
      "  Using cached gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Collecting nltk==3.8.1\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting torchtext==0.6.0\n",
      "  Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk==3.8.1)\n",
      "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Collecting sentencepiece (from torchtext==0.6.0)\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: sentencepiece, regex, nltk, gensim, torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.15.1\n",
      "    Uninstalling torchtext-0.15.1:\n",
      "      Successfully uninstalled torchtext-0.15.1\n",
      "Successfully installed gensim-4.3.1 nltk-3.8.1 regex-2023.10.3 sentencepiece-0.1.99 torchtext-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pre-requisites: \n",
    "# - PyTorch 2.00 kernel\n",
    "# - ml.g5.xlarge instance\n",
    "\n",
    "# Install requirements\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1 torchtext torchdata portalocker | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.15.1 portalocker>=2.0.0 | grep -v \"already satisfied\"\n",
    "!pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.6.0   | grep -v \"already satisfied\"\n",
    "\n",
    "#  torchtext==0.12.0 --> torch==1.11.0\n",
    "#  torchtext==0.13.0 --> torch==1.12.0\n",
    "#  torchtext==0.14.0 --> torch==1.12.0\n",
    "# torchtext==0.15.1 --> torch==2.0.0\n",
    "# torchtext==0.15.2 --> torch==2.0.1\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.10.0 | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.2.0 nltk torchtext  | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import sklearn.model_selection \n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import random\n",
    "import json\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "question_context_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "answer_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    return sentence\n",
    "\n",
    "# def prepare_dataset(df):\n",
    "#     df['context_tokens'] = df['context'].apply(prepare_text)\n",
    "#     df['question_tokens'] = df['question'].apply(prepare_text)\n",
    "#     df['answer_tokens'] = df['answer'].apply(prepare_text)\n",
    "    \n",
    "#     df['context_ids'] = df['context_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['question_ids'] = df['question_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['answer_ids'] = df['answer_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "\n",
    "#     df['full_question_ids'] = df.apply(lambda row: [CONTEXT_index] + row['context_ids'] + [QUESTION_index] + row['question_ids'], axis=1)    \n",
    "\n",
    "# def train_test_split(SRC, TRG, test_size=0.2, random_seed=42):\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "    \n",
    "#     SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset= train_test_split(SRC, TRG, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "#     # Return the training and test datasets\n",
    "#     return SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset\n",
    "\n",
    "\n",
    "def loadDF(path):    \n",
    "    data_file_path = download_from_url(path, root=\"data\")\n",
    "    \n",
    "    with open(data_file_path, 'r') as f:\n",
    "        squad_data = json.load(f)\n",
    "        \n",
    "    data = []\n",
    "    examples = []\n",
    "    for item in squad_data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']                \n",
    "                # question_context = f\"{question} {separator_token} {context}\"\n",
    "                question_context = f\"{question} {context}\"\n",
    "                answer = qa['answers'][0]['text']\n",
    "                \n",
    "                data.append((question_context, answer))                                \n",
    "\n",
    "            #TODO: remove line below\n",
    "            # break    \n",
    "                \n",
    "    df = pd.DataFrame.from_records(data, columns=['question_context', 'answer'])        \n",
    "    df['question_context'] = df['question_context'].apply(prepare_text)\n",
    "    df['answer'] = df['answer'].apply(prepare_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_datasets(df, random_seed=42):\n",
    "    df.to_csv(\"data/data.csv\", index=False)\n",
    "        \n",
    "    fields = [('question_context', question_context_field), ('answer', answer_field)]\n",
    "    dataset = TabularDataset(\"data/data.csv\", format='csv', fields=fields)\n",
    "    \n",
    "    question_context_field.build_vocab(dataset, min_freq=1)\n",
    "    answer_field.build_vocab(dataset, min_freq=1)\n",
    "\n",
    "    train_data, valid_data = dataset.split(split_ratio=0.8, random_state=random.seed(random_seed))\n",
    "    \n",
    "    return train_data, valid_data\n",
    "\n",
    "# def train_test_split(SRC, TRG,  test_size=0.2, random_seed=42):\n",
    "    \n",
    "def tokens_to_string(tokens):\n",
    "    return \" \".join([answer_field.vocab.itos[token] for token in tokens if token != eos_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = loadDF(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "\n",
    "train_data, valid_data = load_datasets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102396\n",
      "42794\n"
     ]
    }
   ],
   "source": [
    "print(len(question_context_field.vocab))\n",
    "\n",
    "print(len(answer_field.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is in front of the Notre Dame Main Buildi...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the Grotto at Notre Dame? Architectura...</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Oregon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>What was Yangon previously known as? Kathmandu...</td>\n",
       "      <td>Rangoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>What is KMC an initialism of? Kathmandu Metrop...</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_context   \n",
       "0      To whom did the Virgin Mary allegedly appear i...  \\\n",
       "1      What is in front of the Notre Dame Main Buildi...   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3      What is the Grotto at Notre Dame? Architectura...   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595  What was Yangon previously known as? Kathmandu...   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598  What is KMC an initialism of? Kathmandu Metrop...   \n",
       "\n",
       "                                        answer  \n",
       "0                   Saint Bernadette Soubirous  \n",
       "1                    a copper statue of Christ  \n",
       "2                            the Main Building  \n",
       "3      a Marian place of prayer and reflection  \n",
       "4           a golden statue of the Virgin Mary  \n",
       "...                                        ...  \n",
       "87594                                   Oregon  \n",
       "87595                                  Rangoon  \n",
       "87596                                    Minsk  \n",
       "87597                                     1975  \n",
       "87598              Kathmandu Metropolitan City  \n",
       "\n",
       "[87599 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What entity provides help with the management of time for new students at Notre Dame? All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a Learning Resource Center which provides time management, collaborative learning, and subject tutoring. This program has been recognized previously, by U.S. News & World Report, as outstanding.\n",
      "Learning Resource Center\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[20]['question_context'])\n",
    "\n",
    "print(df.iloc[20]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "oQLTP2Wmi1eB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)        # How to use it ????\n",
    "        # self.embedding_dim = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        # self.embedding = nn.Embedding(self.input_size, self.embedding_dim)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        embedded = self.embedding(i)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        return output, hidden, cell\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        # self.embedding = nn.Embedding(self.hidden_size, self.hidden_size)  # From lesson\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.embedding = nn.Embedding(hidden_size, embedding_size) # Why ?\n",
    "        \n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # print('[Decoder] input shape:', input.shape)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell)) \n",
    "        \n",
    "        \n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        \n",
    "        print(\"prediction\", prediction)\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):              \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "                \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        input = trg[0, :]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # get highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            print(\"Most likely token: \", top1)\n",
    "            \n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            input = trg[t] if use_teacher_forcing else top1\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "\n",
    "class Seq2SeqInference(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder, answer_field):\n",
    "        \n",
    "        super(Seq2SeqInference, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, max_length=20):\n",
    "        #src = [src len, 1]\n",
    "        #trg = [trg len, 1]\n",
    "        # expected output shape = [max_length, 1]\n",
    "        \n",
    "        if src.shape[1] != 1:\n",
    "            raise ValueError(f\"src.shape[1] != 1: {src.shape[1]}\")\n",
    "            \n",
    "        batch_size = 1 # src.shape[1]\n",
    "\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                                   \n",
    "        logits = torch.zeros(max_length + 1, batch_size, trg_vocab_size).to(device)\n",
    "        # outputs = torch.zeros(max_length + 1, batch_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        # TODO: create array of <sos> tokens      \n",
    "        sos_idx = answer_field.vocab.stoi[answer_field.init_token] \n",
    "        \n",
    "        # input = trg[0, :]\n",
    "        \n",
    "        # print(\"input shape\", trg.shape)\n",
    "        input = torch.tensor([sos_idx]).to(device)        \n",
    "        print(\"Start token: \", input)\n",
    "        \n",
    "        inferred_tokens = []\n",
    "                \n",
    "        # for t in range(1, trg_len):\n",
    "        for t in range(1, max_length + 1):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            logits[t] = output\n",
    "            \n",
    "            # get highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # print(f\"Top1: {top1} {top1.item()}\")\n",
    "            print(f\"Most likely token: {top1.item()} ({answer_field.vocab.itos[top1.item()]})\")\n",
    "            inferred_tokens.append(top1)\n",
    "                        \n",
    "            input = top1                        \n",
    "        \n",
    "        inferred_tokens_tensor = torch.tensor(inferred_tokens).view(-1, 1)\n",
    "        \n",
    "        logits_dim = logits.shape[-1]\n",
    "        logits = logits[1:].view(-1, logits_dim)\n",
    "        \n",
    "        return inferred_tokens_tensor, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Best hyperparameters: {'encoder_dropout': 0.5446679996568586, 'decoder_dropout': 0.24195810605904913, 'embedding_size': 362, 'hidden_size': 1602, 'num_layers': 4, 'learning_rate': 0.0002977109963568765, 'batch_size': 85}\n",
    "\n",
    "# Define the model and other parameters\n",
    "encoder_input_size = len(question_context_field.vocab) \n",
    "encoder_embedding_size= 362 #300\n",
    "encoder_dropout = 0.5446679996568586 #0.5\n",
    "\n",
    "decoder_output_size = len(answer_field.vocab)\n",
    "decoder_embedding_size = 362 #300\n",
    "decoder_dropout = 0.24195810605904913 #0.5\n",
    "\n",
    "hidden_size = 1602 #512\n",
    "num_layers = 4 #2\n",
    "batch_size = 85 #128\n",
    "\n",
    "learning_rate = 0.0002977109963568765 #0.001\n",
    "num_epochs =  20\n",
    "\n",
    "encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "   batch_size=batch_size,\n",
    "   sort_within_batch=True,\n",
    "    sort_key = lambda x: len(x.question_context),\n",
    "    device=device)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# learning_rate = 0.001\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# num_epochs =  20\n",
    "clip = 1\n",
    "valid_loss_min = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    avg_train_loss = 0.0\n",
    "    # epoch_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "        \n",
    "#         print('src:', src.shape)\n",
    "#         print('trg:', trg.shape)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print (src.shape)\n",
    "        # print (trg.shape)\n",
    "        \n",
    "        # Pass the source sequences through the encoder\n",
    "        output = model(src, trg)        #  [trg length, batch size, output dim]\n",
    "        # print('output:', output.shape)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "#         print('output 2:', output.shape)\n",
    "#         print('trg 2:', trg.shape)\n",
    "        \n",
    "#         print('output content:', output)\n",
    "#         print('trg content:', trg)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(train_loss)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # break\n",
    "        \n",
    "    average_train_loss = train_loss / len(train_iterator)\n",
    "    \n",
    "    end_time = time.time()    \n",
    "    train_elapsed_time = end_time - start_time\n",
    "    \n",
    "    # print(f'Epoch: {epoch+1:02} | Train Time: {train_elapsed_time}s')\n",
    "        \n",
    "    # Evaluation on the test dataset\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(valid_iterator):\n",
    "            src = batch.question_context.to(device)\n",
    "            trg = batch.answer.to(device)                    \n",
    "            \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "        average_val_loss = valid_loss / len(valid_iterator)\n",
    "                    \n",
    "    end_time = time.time()    \n",
    "    val_elapsed_time = end_time - start_time\n",
    "    \n",
    "    # print(f'Eval Time: {elapsed_time}')\n",
    "    # print(f'Epoch: {epoch+1:02} | Eval Time: {val_elapsed_time}s')\n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, f'checkpoints/train-{epoch}.pt')\n",
    "    \n",
    "    if valid_loss_min is None or (\n",
    "            (valid_loss_min - average_val_loss) / valid_loss_min > 0.01\n",
    "    ):\n",
    "        print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "        \n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, 'checkpoints/best_val_loss.pt')\n",
    "\n",
    "        valid_loss_min = average_val_loss\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1:02}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f} Train Time: {train_elapsed_time}s Eval Time: {val_elapsed_time}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,      2,      2,  ...,      2,      2,      2],\n",
      "        [    27,     27,      4,  ...,     27,     27, 101321],\n",
      "        [    22,     12,   3956,  ...,     12,     13,      3],\n",
      "        ...,\n",
      "        [   313,    133,    316,  ...,      1,      1,      1],\n",
      "        [     7,      7,      7,  ...,      1,      1,      1],\n",
      "        [     3,      3,      3,  ...,      1,      1,      1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(valid_iterator))\n",
    "\n",
    "text_batch = batch.question_context\n",
    "label_batch = batch.answer\n",
    "\n",
    "print(text_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "test_decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "test_model = Seq2SeqInference(test_encoder, test_decoder, answer_field)\n",
    "test_model.to(device)\n",
    "\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "test_criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # https://pytorch.org/docs/2.0/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "test_criterion.to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/best_val_loss.pt')\n",
    "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "test_iterator = BucketIterator(\n",
    "        # dataset=valid_data,\n",
    "        dataset=train_data,\n",
    "       batch_size=1,\n",
    "       sort_within_batch=True,\n",
    "        sort_key = lambda x: len(x.question_context),\n",
    "        device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[  2],\n",
      "        [280],\n",
      "        [  3]], device='cuda:0')\n",
      "torch.Size([3])\n",
      "tensor([  2, 280,   3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_iterator))\n",
    "\n",
    "text_batch = batch.question_context\n",
    "label_batch = batch.answer\n",
    "\n",
    "print(label_batch.shape)\n",
    "print(label_batch)\n",
    "\n",
    "label_batch = label_batch.squeeze(dim=1)\n",
    "\n",
    "print(label_batch.shape)\n",
    "print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(answer_field.vocab.itos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "        \n",
    "        print(\"src shape\", src.shape)\n",
    "        # print(\"src\", src)\n",
    "        # print(\"trg\", trg)\n",
    "        \n",
    "        tokens, logits = test_model(src, 5)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #logits = [max length, batch, output dim]\n",
    "\n",
    "#         print(\"logits shape\", logits.shape)\n",
    "#         print(\"seq2seq-logits-raw\", logits)\n",
    "        \n",
    "#         logits_dim = logits.shape[-1]\n",
    "#         logits = logits[1:].view(-1, logits_dim)\n",
    "        \n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        print(\"seq2seq-logits\", logits)\n",
    "        print(\"seq2seq-trg\", trg)\n",
    "        print(\"tokens shape\", tokens.shape)\n",
    "        print(\"tokens\", tokens)\n",
    "                \n",
    "        response_text = tokens_to_string(tokens.squeeze().tolist())\n",
    "        \n",
    "        print(response_text)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        # TODO: question: should 'output' really be the MLP result tensor ?\n",
    "#         loss = test_criterion(output, trg)\n",
    "\n",
    "#         valid_loss += loss.item()\n",
    "        \n",
    "        break # TODO: remove line\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against the Validation (or Test ?) Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "test_decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "test_model = Seq2Seq(test_encoder, test_decoder)\n",
    "test_model.to(device)\n",
    "\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "test_criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # https://pytorch.org/docs/2.0/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "test_criterion.to(device)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/best_val_loss.pt')\n",
    "test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "test_iterator = BucketIterator(\n",
    "        # dataset=valid_data,\n",
    "        dataset=train_data,\n",
    "       batch_size=1,\n",
    "       sort_within_batch=True,\n",
    "        sort_key = lambda x: len(x.question_context),\n",
    "        device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "tensor([[   2],\n",
      "        [2987],\n",
      "        [  16],\n",
      "        [   3]], device='cuda:0')\n",
      "<sos>\n",
      "consumption\n",
      "of\n",
      "carcinogenic\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_iterator))\n",
    "\n",
    "question_context_item = batch.question_context\n",
    "answer_item = batch.answer\n",
    "\n",
    "# print(question_context_item.shape)\n",
    "# print(question_context_item)\n",
    "print(answer_item.shape)\n",
    "print(answer_item)\n",
    "\n",
    "# print(answer_field.vocab.itos[2])\n",
    "# print(answer_field.vocab.itos[1324])\n",
    "# print(answer_field.vocab.itos[2015])\n",
    "# print(answer_field.vocab.itos[238])\n",
    "# print(answer_field.vocab.itos[3])\n",
    "\n",
    "print(answer_field.vocab.itos[2])\n",
    "print(answer_field.vocab.itos[1568])\n",
    "print(answer_field.vocab.itos[5])\n",
    "print(answer_field.vocab.itos[24849])\n",
    "print(answer_field.vocab.itos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Notre', 'Dame']\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# TEXT.postprocess(tokenized_text)\n",
    "\n",
    "# question_context_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "# answer_field = Field(tokenize=word_tokenize, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n",
    "input_text = \"Notre Dame\"\n",
    "\n",
    "# tokenized_text = question_context_field.preprocess(\"Notre Dame\")\n",
    "\n",
    "tokenized_text = question_context_field.tokenize(input_text)  # Tokenize the input text\n",
    "input_ids = [question_context_field.vocab.stoi[token] for token in tokenized_text]  # Convert tokens to IDs\n",
    "\n",
    "# Output the tokenized text (list of tokens)\n",
    "print(tokenized_text)\n",
    "print(input_ids)\n",
    "\n",
    "# Convert tokens back to a string using the Field's postprocessing function\n",
    "# reconstructed_text = question_context_field.postprocess(tokenized_text)\n",
    "# print(reconstructed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape torch.Size([72, 1])\n",
      "trg tensor([[  2],\n",
      "        [652],\n",
      "        [ 11],\n",
      "        [  3]], device='cuda:0')\n",
      "prediction tensor([[-4.8642, -4.7556, -4.7049,  ..., -3.7168, -4.2894, -4.2225]],\n",
      "       device='cuda:0')\n",
      "Most likely token:  tensor([4], device='cuda:0')\n",
      "prediction tensor([[-5.8746, -5.8165, -5.7985,  ..., -4.1126, -5.4001, -4.5968]],\n",
      "       device='cuda:0')\n",
      "Most likely token:  tensor([3], device='cuda:0')\n",
      "prediction tensor([[-6.4086, -6.2010, -6.2469,  ..., -4.8947, -5.8284, -4.6923]],\n",
      "       device='cuda:0')\n",
      "Most likely token:  tensor([3], device='cuda:0')\n",
      "output shape torch.Size([4, 1, 42794])\n",
      "seq2seq-output-raw tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-4.8642, -4.7556, -4.7049,  ..., -3.7168, -4.2894, -4.2225]],\n",
      "\n",
      "        [[-5.8746, -5.8165, -5.7985,  ..., -4.1126, -5.4001, -4.5968]],\n",
      "\n",
      "        [[-6.4086, -6.2010, -6.2469,  ..., -4.8947, -5.8284, -4.6923]]],\n",
      "       device='cuda:0')\n",
      "seq2seq-output tensor([[-4.8642, -4.7556, -4.7049,  ..., -3.7168, -4.2894, -4.2225],\n",
      "        [-5.8746, -5.8165, -5.7985,  ..., -4.1126, -5.4001, -4.5968],\n",
      "        [-6.4086, -6.2010, -6.2469,  ..., -4.8947, -5.8284, -4.6923]],\n",
      "       device='cuda:0')\n",
      "seq2seq-trg tensor([652,  11,   3], device='cuda:0')\n",
      "Val Loss: 0.026 Eval Time: 0.09170413017272949s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_iterator):\n",
    "        src = batch.question_context.to(device)\n",
    "        trg = batch.answer.to(device)\n",
    "\n",
    "        print(\"src shape\", src.shape)\n",
    "        # print(\"src\", src)\n",
    "        print(\"trg\", trg)\n",
    "        \n",
    "        output = test_model(src, trg, teacher_forcing_ratio=0) #turn off teacher forcing\n",
    "\n",
    "        # print(output)\n",
    "        # print (\"output\", output)\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        print(\"output shape\", output.shape)\n",
    "        print(\"seq2seq-output-raw\", output)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        print(\"seq2seq-output\", output)\n",
    "        print(\"seq2seq-trg\", trg)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        # TODO: question: should 'output' really be the MLP result tensor ?\n",
    "        loss = test_criterion(output, trg)\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        break # TODO: remove line\n",
    "\n",
    "    average_val_loss = valid_loss / len(valid_iterator)\n",
    "\n",
    "end_time = time.time()    \n",
    "val_elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Val Loss: {average_val_loss:.3f} Eval Time: {val_elapsed_time}s\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(102396, 362)\n",
       "    (lstm): LSTM(362, 1602, num_layers=4, dropout=0.5446679996568586)\n",
       "    (dropout): Dropout(p=0.5446679996568586, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(42794, 362)\n",
       "    (lstm): LSTM(362, 1602, num_layers=4, dropout=0.24195810605904913)\n",
       "    (fc): Linear(in_features=1602, out_features=42794, bias=True)\n",
       "    (dropout): Dropout(p=0.24195810605904913, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy==1.4.8\n",
      "  Using cached SQLAlchemy-1.4.8-cp310-cp310-linux_x86_64.whl\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.6/409.6 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.8/226.8 kB 4.5 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0 sqlalchemy-1.4.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  sqlalchemy==1.4.8  optuna | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import orm\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "encoder_input_size = len(question_context_field.vocab) \n",
    "decoder_output_size = len(answer_field.vocab)\n",
    "pad_idx = answer_field.vocab.stoi[answer_field.pad_token] \n",
    "\n",
    "def objective(trial):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    encoder_dropout = trial.suggest_float('encoder_dropout', 0.1, 0.9)\n",
    "    decoder_dropout = trial.suggest_float('decoder_dropout', 0.1, 0.9)\n",
    "    embedding_size = trial.suggest_int('embedding_size', 200, 512)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 64, 2048)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    \n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    # weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    # opt = trial.suggest_categorical('opt', ['sgd', 'adam'])\n",
    "    # momentum = trial.suggest_uniform('momentum', 0.1, 0.9)\n",
    "    \n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128)\n",
    "    \n",
    "    print(f'Starting trial {trial.number}:\\tHyperparameters={trial.params}') \n",
    "    \n",
    "    train_iterator, valid_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data),\n",
    "       batch_size=batch_size,\n",
    "       sort_within_batch=True,\n",
    "        sort_key = lambda x: len(x.question_context),\n",
    "        device=device)\n",
    "    \n",
    "    encoder = Encoder(encoder_input_size, hidden_size, embedding_size, num_layers, encoder_dropout)\n",
    "    decoder = Decoder(hidden_size, decoder_output_size, embedding_size, num_layers, decoder_dropout)\n",
    "    model = Seq2Seq(encoder, decoder).to(device)    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) # https://pytorch.org/docs/2.0/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_epochs = 5\n",
    "    clip = 1\n",
    "    valid_loss_min = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        avg_train_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            src = batch.question_context.to(device)\n",
    "            trg = batch.answer.to(device)\n",
    "\n",
    "    #         print('src:', src.shape)\n",
    "    #         print('trg:', trg.shape)        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # print (src.shape)\n",
    "            # print (trg.shape)\n",
    "\n",
    "            # Pass the source sequences through the encoder\n",
    "            output = model(src, trg)        #  [trg length, batch size, output dim]\n",
    "            # print('output:', output.shape)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "    #         print('output 2:', output.shape)\n",
    "    #         print('trg 2:', trg.shape)\n",
    "\n",
    "    #         print('output content:', output)\n",
    "    #         print('trg content:', trg)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(train_loss)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "        average_train_loss = train_loss / len(train_iterator)\n",
    "\n",
    "        end_time = time.time()    \n",
    "        train_elapsed_time = end_time - start_time\n",
    "\n",
    "        # print(f'Epoch: {epoch+1:02} | Train Time: {train_elapsed_time}s')\n",
    "\n",
    "        # Evaluation on the test dataset\n",
    "        start_time = time.time()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            for batch_idx, batch in enumerate(valid_iterator):\n",
    "                src = batch.question_context.to(device)\n",
    "                trg = batch.answer.to(device)                    \n",
    "\n",
    "                output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "                #trg = [trg len, batch size]\n",
    "                #output = [trg len, batch size, output dim]\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = trg[1:].view(-1)\n",
    "\n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            average_val_loss = valid_loss / len(valid_iterator)\n",
    "\n",
    "        end_time = time.time()    \n",
    "        val_elapsed_time = end_time - start_time\n",
    "\n",
    "        # print(f'Eval Time: {elapsed_time}')\n",
    "        # print(f'Epoch: {epoch+1:02} | Eval Time: {val_elapsed_time}s')\n",
    "\n",
    "        if valid_loss_min is None or (\n",
    "                (valid_loss_min - average_val_loss) / valid_loss_min > 0.01\n",
    "        ):\n",
    "            print(f\"New minimum validation loss: {average_val_loss:.6f}. Saving model ...\")\n",
    "\n",
    "            torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': train_loss,\n",
    "                }, f'checkpoints/hpo_{trial.number}_model.pt\",')\n",
    "\n",
    "            valid_loss_min = average_val_loss\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:02}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f} Train Time: {train_elapsed_time}s Eval Time: {val_elapsed_time}s\")    \n",
    "        \n",
    "        return valid_loss_min\n",
    "    \n",
    "# Define study and optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(objective, n_trials=100, catch=(torch.cuda.OutOfMemoryError,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: FrozenTrial(number=38, state=TrialState.COMPLETE, values=[6.859984294227932], datetime_start=datetime.datetime(2023, 7, 17, 13, 41, 28, 407974), datetime_complete=datetime.datetime(2023, 7, 17, 13, 54, 17, 155683), params={'encoder_dropout': 0.5446679996568586, 'decoder_dropout': 0.24195810605904913, 'embedding_size': 362, 'hidden_size': 1602, 'num_layers': 4, 'learning_rate': 0.0002977109963568765, 'batch_size': 85}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'encoder_dropout': FloatDistribution(high=0.9, log=False, low=0.1, step=None), 'decoder_dropout': FloatDistribution(high=0.9, log=False, low=0.1, step=None), 'embedding_size': IntDistribution(high=512, log=False, low=200, step=1), 'hidden_size': IntDistribution(high=2048, log=False, low=64, step=1), 'num_layers': IntDistribution(high=4, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'batch_size': IntDistribution(high=128, log=False, low=32, step=1)}, trial_id=38, value=None)\n",
      "Best hyperparameters: {'encoder_dropout': 0.5446679996568586, 'decoder_dropout': 0.24195810605904913, 'embedding_size': 362, 'hidden_size': 1602, 'num_layers': 4, 'learning_rate': 0.0002977109963568765, 'batch_size': 85}\n",
      "Validation accuracy: 6.859984294227932\n"
     ]
    }
   ],
   "source": [
    "print('Best trial:', study.best_trial)\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Validation loss:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# Epoch: 01, Train Loss: 6.988, Val Loss: 6.898\n",
    "\n",
    "#  Round 1\n",
    "# Best hyperparameters: {'encoder_dropout': 0.6030214714084597, 'decoder_dropout': 0.2892322144702524, 'embedding_size': 203, 'hidden_size': 1662, 'num_layers': 2, 'learning_rate': 2.4979480026780982e-05, 'batch_size': 82}\n",
    "# Validation accuracy: 6.920195218558623\n",
    "\n",
    "#  Round 2\n",
    "# Best hyperparameters: {'encoder_dropout': 0.5446679996568586, 'decoder_dropout': 0.24195810605904913, 'embedding_size': 362, 'hidden_size': 1602, 'num_layers': 4, 'learning_rate': 0.0002977109963568765, 'batch_size': 85}\n",
    "# Validation loss: 6.859984294227932\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2],\n",
      "        [1818],\n",
      "        [ 124],\n",
      "        [   6],\n",
      "        [1289],\n",
      "        [   3]], device='cuda:0')\n",
      "mexico\n"
     ]
    }
   ],
   "source": [
    "# print(batch.question_context)\n",
    "\n",
    "print(batch.answer)\n",
    "\n",
    "print(answer_field.vocab.itos[980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
