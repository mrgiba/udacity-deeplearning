{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Steps Overview and Estimated Duration\n",
    "# Below you will find each of the components of the project, and estimated times to complete each portion. \n",
    "# These are estimates and not exact timings to help you expect the amount of time necessary to put aside to work on your project.\n",
    "\n",
    "# Prepare data (~2 hours)\n",
    "# Build your vocabulary from a corpus of language data. The Vocabulary object is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Build Model (~4 hours)\n",
    "# Build your Encoder, Decoder, and larger Sequence to Sequence pattern in PyTorch. This pattern is described in Lesson Six: Seq2Seq.\n",
    "\n",
    "# Train Model (~3 hours)\n",
    "# Write your training procedure and divide your dataset into train/test/validation splits. Then, train your network and plot your evaluation metrics. Save your model after it reaches a satisfactory level of accuracy.\n",
    "\n",
    "# Evaluate & Interact w/ Model (~1 hour)\n",
    "# Write a script to interact with your network at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Summary\n",
    "# The LSTM Chatbot will help you show off your skills as a deep learning practitioner. You will develop the chatbot using a new architecture called a Seq2Seq. \n",
    "# Additionally, you can use pre-trained word embeddings to improve the performance of your model. Let's get started by following the steps below:\n",
    "\n",
    "# Step 1: Build your Vocabulary & create the Word Embeddings\n",
    "# The most important part of this step is to create your Vocabulary object using a corpus of data drawn from TorchText.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Use Gensim to extract the word embeddings from one of its corpus'.\n",
    "# Use NLTK and Gensim to create a function to clean your text and look up the index of a word's embeddings.\n",
    "\n",
    "# Step 2: Create the Encoder\n",
    "# A Seq2Seq architecture consists of an encoder and a decoder unit. You will use Pytorch to build a full Seq2Seq model.\n",
    "# The first step of the architecture is to create an encoder with an LSTM unit.\n",
    "\n",
    "# (Extra Credit)\n",
    "# Load your pretrained embeddings into the LSTM unit.\n",
    "\n",
    "# Step 3: Create the Decoder\n",
    "# The second step of the architecture is to create a decoder using a second LSTM unit.\n",
    "\n",
    "# Step 4: Combine them into a Seq2Seq Architecture\n",
    "# To finalize your model, you will combine the encoder and decoder units into a working model.\n",
    "# The Seq2Seq2 model must be able to instantiate the encoder and decoder. Then, it will accept the inputs for these units and manage their interaction to get an output using the forward pass function.\n",
    "\n",
    "# Step 5: Train & evaluate your model\n",
    "# Finally you will train and evaluate your model using a Pytorch training loop.\n",
    "\n",
    "# Step 6: Interact with the Chatbot\n",
    "# Demonstrate your chatbot by converting the outputs of the model to text and displaying it's responses at the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Pre-requisite: Select PyTorch 2.00 kernel\n",
    "\n",
    "# Install requirements\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1 torchtext torchdata portalocker | grep -v \"already satisfied\"\n",
    "\n",
    "!pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.15.1 portalocker>=2.0.0 | grep -v \"already satisfied\"\n",
    "\n",
    "#  torchtext==0.12.0 --> torch==1.11.0\n",
    "#  torchtext==0.13.0 --> torch==1.12.0\n",
    "#  torchtext==0.14.0 --> torch==1.12.0\n",
    "# torchtext==0.15.1 --> torch==2.0.0\n",
    "# torchtext==0.15.2 --> torch==2.0.1\n",
    "\n",
    "# !pip install gensim==4.3.1 nltk==3.8.1  torchtext==0.10.0 | grep -v \"already satisfied\"\n",
    "\n",
    "# !pip install gensim==4.2.0 nltk torchtext  | grep -v \"already satisfied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.1\n"
     ]
    }
   ],
   "source": [
    "# from torchtext.legacy import data\n",
    "\n",
    "import torchtext\n",
    "\n",
    "print(torchtext.__version__)\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "# from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "PAD_index = 0  # Used for padding short sentences\n",
    "SOS_index = 1  # Start-of-sentence token\n",
    "EOS_index = 2  # End-of-sentence token\n",
    "CONTEXT_index = 3  # Used for padding short sentences\n",
    "QUESTION_index = 4  # Used for padding short sentences\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):        \n",
    "        self.word2index = {}\n",
    "        # self.index2word = []\n",
    "        self.index2word =  [ \"<pad>\", \"<sos>\",  \"<eos>\",  \"<context>\", \"<question>\"]\n",
    "\n",
    "    def load(self, words):\n",
    "        for word in words:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        return text\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = len(self.index2word)\n",
    "            self.index2word.append(word)\n",
    "\n",
    "    def get_index(self, word):\n",
    "        return self.word2index.get(word, None)\n",
    "\n",
    "    def get_word(self, index):\n",
    "        if index >= 0 and index < len(self.index2word):\n",
    "            return self.index2word[index]\n",
    "        return None\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.index2word)\n",
    "    \n",
    "    \n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question_ids = self.data.loc[index, 'question_ids']\n",
    "        answer_ids = self.data.loc[index, 'answer_ids']\n",
    "        \n",
    "        return torch.tensor(question_ids), torch.tensor(answer_ids)\n",
    "    \n",
    "    \n",
    "class VocabularyNew:\n",
    "    def __init__(self, name): \n",
    "        self.name = name\n",
    "        self.index = {}   # Index to word     \n",
    "        self.words = {}  # Word to index\n",
    "        self.count = 0\n",
    "\n",
    "def clean_text(self, text):\n",
    "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\ttext = tokenizer.tokenize(text)\n",
    "\treturn text\n",
    "\n",
    "def index_word(self, word):\n",
    "\tif word not in self.words:\n",
    "\t\tself.words[word] = self.count\n",
    "\t\tself.index[str(self.count)] = word\n",
    "\t\tself.count += 1\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "    \n",
    "vocabulary = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torchtext.datasets import SQuAD1\n",
    "import sklearn.model_selection \n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "\n",
    "# w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "\n",
    "SQUAD1_CONTEXT_INDEX = 0\n",
    "SQUAD1_QUESTION_INDEX = 1\n",
    "SQUAD1_ANSWER_INDEX = 2\n",
    "\n",
    "def loadDF(path):    \n",
    "    torchtext.utils.download_from_url(path, root=\"data\")\n",
    "    dataset = SQuAD1(root=\"data\", split=\"train\")\n",
    "    \n",
    "    data = [\n",
    "        (row[SQUAD1_CONTEXT_INDEX], row[SQUAD1_QUESTION_INDEX], row[SQUAD1_ANSWER_INDEX][0]) for row in dataset\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame.from_records(data, columns=['context', 'question', 'answer'])    \n",
    "    \n",
    "    df['context_tokens'] = df['context'].apply(prepare_text)\n",
    "    df['question_tokens'] = df['question'].apply(prepare_text)\n",
    "    df['answer_tokens'] = df['answer'].apply(prepare_text)        \n",
    "        \n",
    "    df['context_ids'] = df['context_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "    df['question_ids'] = df['question_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "    df['answer_ids'] = df['answer_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "\n",
    "    df['full_question_ids'] = df.apply(lambda row: [CONTEXT_index] + row['context_ids'] + [QUESTION_index] + row['question_ids'], axis=1)    \n",
    "    \n",
    "    #  Should append  <sos> and <eos>  ?\n",
    "    df['full_question_ids'] = df['full_question_ids'].apply(lambda x: [SOS_index] + x + [EOS_index])\n",
    "    df['answer_ids'] = df['answer_ids'].apply(lambda x: [SOS_index] + x + [EOS_index])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_text(sentence):    \n",
    "    # By default, the word_tokenize function in NLTK converts all words to lowercase    \n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    vocabulary.load(tokens)    \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# def prepare_dataset(df):\n",
    "#     df['context_tokens'] = df['context'].apply(prepare_text)\n",
    "#     df['question_tokens'] = df['question'].apply(prepare_text)\n",
    "#     df['answer_tokens'] = df['answer'].apply(prepare_text)\n",
    "    \n",
    "#     df['context_ids'] = df['context_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['question_ids'] = df['question_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "#     df['answer_ids'] = df['answer_tokens'].apply(lambda x: [vocabulary.get_index(token) for token in x])\n",
    "\n",
    "#     df['full_question_ids'] = df.apply(lambda row: [CONTEXT_index] + row['context_ids'] + [QUESTION_index] + row['question_ids'], axis=1)    \n",
    "\n",
    "def train_test_split(SRC, TRG, test_size=0.2, random_seed=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset= train_test_split(SRC, TRG, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "    # Return the training and test datasets\n",
    "    return SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset\n",
    "\n",
    "# def train_test_split(SRC, TRG,  test_size=0.2, random_seed=42):\n",
    "    \n",
    "#     '''\n",
    "#     Input: SRC, our list of questions from the dataset\n",
    "#             TRG, our list of responses from the dataset\n",
    "\n",
    "#     Output: Training and test datasets for SRC & TRG\n",
    "\n",
    "#     '''\n",
    "# #     np.random.seed(random_seed)\n",
    "# #     indices = np.arange(len(SRC))\n",
    "# #     np.random.shuffle(indices)\n",
    "# #     split_idx = int(len(indices) * (1 - test_size))\n",
    "    \n",
    "# #     SRC_train_dataset = [SRC[i] for i in indices[:split_idx]]\n",
    "# #     SRC_test_dataset = [SRC[i] for i in indices[split_idx:]]\n",
    "# #     TRG_train_dataset = [TRG[i] for i in indices[:split_idx]]\n",
    "# #     TRG_test_dataset = [TRG[i] for i in indices[split_idx:]]\n",
    "    \n",
    "#     SRC_train, SRC_test, TRG_train, TRG_test = sklearn.model_selection.train_test_split(SRC, TRG, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "#     # Convert the training and test datasets into PyTorch tensors\n",
    "#     SRC_train_dataset = Seq2SeqDataset(pd.DataFrame({'question_ids': SRC_train, 'answer_ids': TRG_train}))\n",
    "#     SRC_test_dataset = Seq2SeqDataset(pd.DataFrame({'question_ids': SRC_test, 'answer_ids': TRG_test}))\n",
    "    \n",
    "    \n",
    "#     return SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = loadDF(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_tokens</th>\n",
       "      <th>question_tokens</th>\n",
       "      <th>answer_tokens</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>answer_ids</th>\n",
       "      <th>full_question_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>[Architecturally, ,, the, school, has, a, Cath...</td>\n",
       "      <td>[To, whom, did, the, Virgin, Mary, allegedly, ...</td>\n",
       "      <td>[Saint, Bernadette, Soubirous]</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...</td>\n",
       "      <td>[2918, 2814, 3023, 7, 24, 25, 9089, 3334, 27, ...</td>\n",
       "      <td>[1, 65, 66, 67, 2]</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>[Architecturally, ,, the, school, has, a, Cath...</td>\n",
       "      <td>[What, is, in, front, of, the, Notre, Dame, Ma...</td>\n",
       "      <td>[a, copper, statue, of, Christ]</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...</td>\n",
       "      <td>[2289, 20, 27, 28, 23, 7, 91, 92, 15, 16, 1505]</td>\n",
       "      <td>[1, 10, 32, 22, 23, 33, 2]</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>[Architecturally, ,, the, school, has, a, Cath...</td>\n",
       "      <td>[The, Basilica, of, the, Sacred, heart, at, No...</td>\n",
       "      <td>[the, Main, Building]</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...</td>\n",
       "      <td>[99, 46, 23, 7, 47, 7692, 59, 91, 92, 20, 4475...</td>\n",
       "      <td>[1, 7, 15, 16, 2]</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>[Architecturally, ,, the, school, has, a, Cath...</td>\n",
       "      <td>[What, is, the, Grotto, at, Notre, Dame, ?]</td>\n",
       "      <td>[a, Marian, place, of, prayer, and, reflection]</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...</td>\n",
       "      <td>[2289, 20, 7, 51, 59, 91, 92, 1505]</td>\n",
       "      <td>[1, 10, 52, 53, 23, 54, 29, 55, 2]</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>[Architecturally, ,, the, school, has, a, Cath...</td>\n",
       "      <td>[What, sits, on, top, of, the, Main, Building,...</td>\n",
       "      <td>[a, golden, statue, of, the, Virgin, Mary]</td>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...</td>\n",
       "      <td>[2289, 23850, 135, 500, 23, 7, 15, 16, 59, 91,...</td>\n",
       "      <td>[1, 10, 21, 22, 23, 7, 24, 25, 2]</td>\n",
       "      <td>[1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>[Kathmandu, Metropolitan, City, (, KMC, ), ,, ...</td>\n",
       "      <td>[In, what, US, state, did, Kathmandu, first, e...</td>\n",
       "      <td>[Oregon]</td>\n",
       "      <td>[109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...</td>\n",
       "      <td>[166, 3581, 2799, 2325, 3023, 109309, 322, 340...</td>\n",
       "      <td>[1, 4434, 2]</td>\n",
       "      <td>[1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>Rangoon</td>\n",
       "      <td>[Kathmandu, Metropolitan, City, (, KMC, ), ,, ...</td>\n",
       "      <td>[What, was, Yangon, previously, known, as, ?]</td>\n",
       "      <td>[Rangoon]</td>\n",
       "      <td>[109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...</td>\n",
       "      <td>[2289, 179, 71358, 351, 472, 113, 1505]</td>\n",
       "      <td>[1, 71359, 2]</td>\n",
       "      <td>[1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>Minsk</td>\n",
       "      <td>[Kathmandu, Metropolitan, City, (, KMC, ), ,, ...</td>\n",
       "      <td>[With, what, Belorussian, city, does, Kathmand...</td>\n",
       "      <td>[Minsk]</td>\n",
       "      <td>[109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...</td>\n",
       "      <td>[916, 3581, 81077, 4133, 157, 109309, 142, 10,...</td>\n",
       "      <td>[1, 78279, 2]</td>\n",
       "      <td>[1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "      <td>[Kathmandu, Metropolitan, City, (, KMC, ), ,, ...</td>\n",
       "      <td>[In, what, year, did, Kathmandu, create, its, ...</td>\n",
       "      <td>[1975]</td>\n",
       "      <td>[109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...</td>\n",
       "      <td>[166, 3581, 133, 3023, 109309, 694, 207, 4129,...</td>\n",
       "      <td>[1, 11592, 2]</td>\n",
       "      <td>[1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>Kathmandu Metropolitan City</td>\n",
       "      <td>[Kathmandu, Metropolitan, City, (, KMC, ), ,, ...</td>\n",
       "      <td>[What, is, KMC, an, initialism, of, ?]</td>\n",
       "      <td>[Kathmandu, Metropolitan, City]</td>\n",
       "      <td>[109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...</td>\n",
       "      <td>[2289, 20, 109676, 155, 91372, 23, 1505]</td>\n",
       "      <td>[1, 109309, 4411, 1214, 2]</td>\n",
       "      <td>[1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context   \n",
       "0      Architecturally, the school has a Catholic cha...  \\\n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87598  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                                question   \n",
       "0      To whom did the Virgin Mary allegedly appear i...  \\\n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595               What was Yangon previously known as?   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598                      What is KMC an initialism of?   \n",
       "\n",
       "                                        answer   \n",
       "0                   Saint Bernadette Soubirous  \\\n",
       "1                    a copper statue of Christ   \n",
       "2                            the Main Building   \n",
       "3      a Marian place of prayer and reflection   \n",
       "4           a golden statue of the Virgin Mary   \n",
       "...                                        ...   \n",
       "87594                                   Oregon   \n",
       "87595                                  Rangoon   \n",
       "87596                                    Minsk   \n",
       "87597                                     1975   \n",
       "87598              Kathmandu Metropolitan City   \n",
       "\n",
       "                                          context_tokens   \n",
       "0      [Architecturally, ,, the, school, has, a, Cath...  \\\n",
       "1      [Architecturally, ,, the, school, has, a, Cath...   \n",
       "2      [Architecturally, ,, the, school, has, a, Cath...   \n",
       "3      [Architecturally, ,, the, school, has, a, Cath...   \n",
       "4      [Architecturally, ,, the, school, has, a, Cath...   \n",
       "...                                                  ...   \n",
       "87594  [Kathmandu, Metropolitan, City, (, KMC, ), ,, ...   \n",
       "87595  [Kathmandu, Metropolitan, City, (, KMC, ), ,, ...   \n",
       "87596  [Kathmandu, Metropolitan, City, (, KMC, ), ,, ...   \n",
       "87597  [Kathmandu, Metropolitan, City, (, KMC, ), ,, ...   \n",
       "87598  [Kathmandu, Metropolitan, City, (, KMC, ), ,, ...   \n",
       "\n",
       "                                         question_tokens   \n",
       "0      [To, whom, did, the, Virgin, Mary, allegedly, ...  \\\n",
       "1      [What, is, in, front, of, the, Notre, Dame, Ma...   \n",
       "2      [The, Basilica, of, the, Sacred, heart, at, No...   \n",
       "3            [What, is, the, Grotto, at, Notre, Dame, ?]   \n",
       "4      [What, sits, on, top, of, the, Main, Building,...   \n",
       "...                                                  ...   \n",
       "87594  [In, what, US, state, did, Kathmandu, first, e...   \n",
       "87595      [What, was, Yangon, previously, known, as, ?]   \n",
       "87596  [With, what, Belorussian, city, does, Kathmand...   \n",
       "87597  [In, what, year, did, Kathmandu, create, its, ...   \n",
       "87598             [What, is, KMC, an, initialism, of, ?]   \n",
       "\n",
       "                                         answer_tokens   \n",
       "0                       [Saint, Bernadette, Soubirous]  \\\n",
       "1                      [a, copper, statue, of, Christ]   \n",
       "2                                [the, Main, Building]   \n",
       "3      [a, Marian, place, of, prayer, and, reflection]   \n",
       "4           [a, golden, statue, of, the, Virgin, Mary]   \n",
       "...                                                ...   \n",
       "87594                                         [Oregon]   \n",
       "87595                                        [Rangoon]   \n",
       "87596                                          [Minsk]   \n",
       "87597                                           [1975]   \n",
       "87598                  [Kathmandu, Metropolitan, City]   \n",
       "\n",
       "                                             context_ids   \n",
       "0      [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...  \\\n",
       "1      [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...   \n",
       "2      [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...   \n",
       "3      [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...   \n",
       "4      [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16,...   \n",
       "...                                                  ...   \n",
       "87594  [109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...   \n",
       "87595  [109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...   \n",
       "87596  [109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...   \n",
       "87597  [109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...   \n",
       "87598  [109309, 4411, 1214, 73, 109676, 83, 6, 27, 32...   \n",
       "\n",
       "                                            question_ids   \n",
       "0      [2918, 2814, 3023, 7, 24, 25, 9089, 3334, 27, ...  \\\n",
       "1        [2289, 20, 27, 28, 23, 7, 91, 92, 15, 16, 1505]   \n",
       "2      [99, 46, 23, 7, 47, 7692, 59, 91, 92, 20, 4475...   \n",
       "3                    [2289, 20, 7, 51, 59, 91, 92, 1505]   \n",
       "4      [2289, 23850, 135, 500, 23, 7, 15, 16, 59, 91,...   \n",
       "...                                                  ...   \n",
       "87594  [166, 3581, 2799, 2325, 3023, 109309, 322, 340...   \n",
       "87595            [2289, 179, 71358, 351, 472, 113, 1505]   \n",
       "87596  [916, 3581, 81077, 4133, 157, 109309, 142, 10,...   \n",
       "87597  [166, 3581, 133, 3023, 109309, 694, 207, 4129,...   \n",
       "87598           [2289, 20, 109676, 155, 91372, 23, 1505]   \n",
       "\n",
       "                               answer_ids   \n",
       "0                      [1, 65, 66, 67, 2]  \\\n",
       "1              [1, 10, 32, 22, 23, 33, 2]   \n",
       "2                       [1, 7, 15, 16, 2]   \n",
       "3      [1, 10, 52, 53, 23, 54, 29, 55, 2]   \n",
       "4       [1, 10, 21, 22, 23, 7, 24, 25, 2]   \n",
       "...                                   ...   \n",
       "87594                        [1, 4434, 2]   \n",
       "87595                       [1, 71359, 2]   \n",
       "87596                       [1, 78279, 2]   \n",
       "87597                       [1, 11592, 2]   \n",
       "87598          [1, 109309, 4411, 1214, 2]   \n",
       "\n",
       "                                       full_question_ids  \n",
       "0      [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...  \n",
       "1      [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...  \n",
       "2      [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...  \n",
       "3      [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...  \n",
       "4      [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1...  \n",
       "...                                                  ...  \n",
       "87594  [1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...  \n",
       "87595  [1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...  \n",
       "87596  [1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...  \n",
       "87597  [1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...  \n",
       "87598  [1, 3, 109309, 4411, 1214, 73, 109676, 83, 6, ...  \n",
       "\n",
       "[87599 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_question_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SRC_train_dataset, SRC_val_dataset, TRG_train_dataset, TRG_val_dataset = train_test_split(df['full_question_ids'], df['answer_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41171    [1, 3, 1431, 936, 10, 5160, 521, 64806, 1182, ...\n",
       "11421    [1, 3, 18568, 17, 3429, 27, 7, 4557, 5269, 23,...\n",
       "45787    [1, 3, 56, 9, 349, 6467, 76, 69760, 48509, 754...\n",
       "38917    [1, 3, 1431, 10844, 152, 5572, 6, 6813, 12142,...\n",
       "72419    [1, 3, 1116, 12744, 6, 12611, 17, 4246, 726, 1...\n",
       "                               ...                        \n",
       "6265     [1, 3, 18553, 1083, 12347, 45, 17833, 2980, 17...\n",
       "54886    [1, 3, 79525, 9, 349, 4572, 27, 6174, 268, 210...\n",
       "76820    [1, 3, 97316, 20, 472, 200, 1025, 10, 100929, ...\n",
       "860      [1, 3, 2381, 17, 691, 9, 2458, 1311, 2516, 377...\n",
       "15795    [1, 3, 5841, 7, 1517, 23, 7, 215, 4665, 6, 155...\n",
       "Name: full_question_ids, Length: 70079, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70079"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22/3513352084.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(padded_series_list)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_series_to_tensor(series):\n",
    "    # Convert Pandas Series to list of tensors\n",
    "    series_list = [torch.tensor(lst) for lst in series]\n",
    "\n",
    "    # Pad sequences with zeroes\n",
    "    padded_series_list = pad_sequence(series_list, batch_first=True, padding_value=PAD_index)\n",
    "\n",
    "    # Convert padded sequences to PyTorch tensor\n",
    "    tensor = torch.tensor(padded_series_list)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "src_train_tensor = pad_series_to_tensor(SRC_train_dataset).to(device)\n",
    "trg_train_tensor = pad_series_to_tensor(TRG_train_dataset).to(device)\n",
    "\n",
    "src_val_tensor = pad_series_to_tensor(SRC_val_dataset).to(device)\n",
    "trg_val_tensor = pad_series_to_tensor(TRG_val_dataset).to(device)\n",
    "\n",
    "trg_train_tensor = torch.cat([trg_train_tensor, torch.zeros(trg_train_tensor.size(0), 784 - trg_train_tensor.size(1), dtype=torch.long).to(device)], dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70079, 784])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70079, 784])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     3,  1431,  ...,     0,     0,     0],\n",
       "        [    1,     3, 18568,  ...,     0,     0,     0],\n",
       "        [    1,     3,    56,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    1,     3, 97316,  ...,     0,     0,     0],\n",
       "        [    1,     3,  2381,  ...,     0,     0,     0],\n",
       "        [    1,     3,  5841,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for src, trg in zip(src_train_tensor, src_val_tensor):\n",
    "    display(src.shape)\n",
    "    # display(src)\n",
    "    \n",
    "    src = src.unsqueeze(1)\n",
    "    \n",
    "    display(src.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115678"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "oQLTP2Wmi1eB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.hidden = torch.zeros(1, 1, hidden_size)        # How to use it ????\n",
    "        # self.embedding_dim = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        # self.embedding = nn.Embedding(self.input_size, self.embedding_dim)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, i):\n",
    "        embedded = self.embedding(i)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        \n",
    "        return output, h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        # self.embedding = nn.Embedding(self.hidden_size, self.hidden_size)  # From lesson\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, embedding_size) \n",
    "        # self.embedding = nn.Embedding(hidden_size, embedding_size) # Why ?\n",
    "        \n",
    "        \n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        # self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        # print('[Decoder] input shape:', input.shape)\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "            \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  #  LSTM: Expected input to be 2-D or 3-D but received 4-D tensor\n",
    "        \n",
    "        \n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "        \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder        \n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):              \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "                \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "                       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        \n",
    "        _, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Start with <sos> tokens\n",
    "        input = trg[0, :]  \n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # get highest predicted token\n",
    "            top1 = output.argmax(1)            \n",
    "            \n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            input = trg[t] if use_teacher_forcing else top1\n",
    "    \n",
    "        return outputs\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the model and other parameters\n",
    "encoder_input_size = vocabulary.get_size()\n",
    "encoder_embedding_size= 300\n",
    "# encoder_num_layers = 1\n",
    "encoder_dropout = 0.5\n",
    "\n",
    "decoder_output_size = vocabulary.get_size()\n",
    "# decoder_hidden_size = encoder_hidden_size\n",
    "decoder_embedding_size = 300\n",
    "# decoder_num_layers = 1\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "batch_size = 4\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, embedding_size, num_layers=1, dropout=0):\n",
    "encoder = Encoder(encoder_input_size, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "\n",
    "#     def __init__(self, hidden_size, output_size, embedding_size, num_layers=1, dropout=0):\n",
    "decoder = Decoder(hidden_size, decoder_output_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "    \n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "train_dataset = TensorDataset(src_train_tensor, trg_train_tensor)\n",
    "val_dataset  = TensorDataset(src_val_tensor, trg_val_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Create an instance of your Seq2Seq model\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_index)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs =  20\n",
    "clip = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for src, trg in train_dataloader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # src = src.transpose(0, 1)\n",
    "        # trg = trg.transpose(0, 1)\n",
    "    # for src, trg in zip(src_train_tensor, trg_train_tensor):\n",
    "    \n",
    "#         print(src.shape)\n",
    "#         print(trg.shape)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # src = src.unsqueeze(1)\n",
    "        # trg = trg.unsqueeze(1)\n",
    "        \n",
    "        # print (src.shape)\n",
    "        # print (trg.shape)\n",
    "        \n",
    "        # Pass the source sequences through the encoder\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.item()       \n",
    "        # print(train_loss)\n",
    "        \n",
    "        epoch_loss += train_loss\n",
    "        \n",
    "    # Calculate average epoch loss\n",
    "    average_train_loss = epoch_loss / src_train_tensor.shape[0]\n",
    "    \n",
    "    end_time = time.time()    \n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Train Time: {elapsed_time}')\n",
    "          \n",
    "          \n",
    "    # Evaluation on the test dataset\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for src, trg in val_dataloader:\n",
    "        # for src, trg in zip(src_val_tensor, trg_val_tensor):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            src = src.transpose(0, 1)\n",
    "            trg = trg.transpose(0, 1)\n",
    "            # src = src.unsqueeze(1)\n",
    "            # trg = trg.unsqueeze(1)\n",
    "        \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "        average_val_loss = val_loss / src_val_tensor.shape[0]\n",
    "        \n",
    "    end_time = time.time()    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Eval Time: {elapsed_time}')\n",
    "    \n",
    "    # Print or log the epoch loss and evaluation metric\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {average_train_loss:.3f}, Val Loss: {average_val_loss:.3f}\")\n",
    "    # print(f\"Epoch: {epoch+1}, Train Loss: {average_loss}, Metric: {evaluation_metric}\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     epoch_loss = 0.0\n",
    "    \n",
    "#     for src, trg in zip(SRC_train_dataset, TRG_train_dataset):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Pass the source sequences through the encoder\n",
    "#         encoder_outputs, hidden = model(src, trg)\n",
    "#         decoder_hidden = hidden\n",
    "        \n",
    "#         # Initialize the decoder input with SOS token\n",
    "#         decoder_input = torch.tensor([[SOS_token]])\n",
    "        \n",
    "#         # Iterate over target sequences one timestep at a time\n",
    "#         for t in range(trg.size(0)):\n",
    "#             decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "#             # Calculate loss between decoder output and target\n",
    "#             loss = criterion(decoder_output.squeeze(0), trg[t])\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "            \n",
    "#             # Backpropagation and parameter update\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Update decoder input with the current target token\n",
    "#             decoder_input = trg[t].unsqueeze(0)\n",
    "    \n",
    "#     # Calculate average epoch loss\n",
    "#     average_loss = epoch_loss / len(SRC_train_dataset)\n",
    "    \n",
    "#     # Evaluation on the test dataset\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Perform inference on the test dataset\n",
    "#         # Calculate evaluation metric (e.g., accuracy, BLEU score)\n",
    "#         evaluation_metric = calculate_evaluation_metric(model, SRC_test_dataset, TRG_test_dataset)\n",
    "    \n",
    "#     # Print or log the epoch loss and evaluation metric\n",
    "#     print(f\"Epoch: {epoch+1}, Loss: {average_loss}, Metric: {evaluation_metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Iterate over the training dataset\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs, batch_targets \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtraining_data\u001b[49m:\n\u001b[1;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_model(batch_inputs, batch_targets)\n\u001b[1;32m     36\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs =  40\n",
    "\n",
    "# Training loop\n",
    "def train_model(src_inputs, trg_inputs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(src_inputs, trg_inputs)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, trg_inputs)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for batch_inputs, batch_targets in training_data:\n",
    "        loss = train_model(batch_inputs, batch_targets)\n",
    "        total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss / len(training_data)\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "# After training, you can use the trained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import SQuAD1\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download and initialize Squad1 dataset\n",
    "torchtext.utils.download_from_url(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\", root=\"data\")\n",
    "train_dataset = SQuAD1(root=\"data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: ['Saint Bernadette Soubirous']\n",
      "Answer start: [515]\n",
      "['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', 'France', '?']\n",
      "['Saint', 'Bernadette', 'Soubirous']\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "SQUAD1_QUESTION_INDEX = 1\n",
    "SQUAD1_ANSWER_INDEX = 2\n",
    "\n",
    "# 0: context (string)\n",
    "# 1: question\n",
    "# 2: answer\n",
    "# 3: answer start\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_data = []\n",
    "# new_vocabulary = Vocabulary()\n",
    "\n",
    "\n",
    "for example in train_dataset:\n",
    "    print(\"Context:\", example[0])\n",
    "    print(\"Question:\", example[1])\n",
    "    print(\"Answer:\", example[2])\n",
    "    print(\"Answer start:\", example[3])\n",
    "    \n",
    "    print(word_tokenize(example[1]))\n",
    "    print(word_tokenize(example[2][0]))\n",
    "    \n",
    "    source = word_tokenize(example[1])\n",
    "    target = word_tokenize(example[2][0])\n",
    "    \n",
    "    print(type(source))\n",
    "    print(type(target))\n",
    "    tokenized_data.append({\"source\": source, \"target\": target})\n",
    "    break\n",
    "    \n",
    "for example in train_dataset:\n",
    "    source = word_tokenize(example[1])\n",
    "    target = word_tokenize(example[2][0])\n",
    "    \n",
    "    # new_vocabulary.load(source)\n",
    "    # new_vocabulary.load(target)\n",
    "    \n",
    "    tokenized_data.append({\"source\": source, \"target\": target})\n",
    "    # break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', ['Saint Bernadette Soubirous'], [515])\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "from torchtext.datasets import SQuAD1\n",
    "# from torchtext.datasets import squad1\n",
    "\n",
    "# Download and initialize Squad1 dataset\n",
    "torchtext.utils.download_from_url(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\", root=\"data\")\n",
    "# train_dataset = SQuAD1(root=\"data\", split=\"train\")\n",
    "train_dataset = SQuAD1(root=\"data\", split=\"train\")\n",
    "# train_dataset = squad1.SQuAD1(root=\"data\", split=\"train\")\n",
    "\n",
    "def tokenize(label, line):\n",
    "    return line.split()\n",
    "\n",
    "# tokens = []\n",
    "# for label, line in train_dataset:\n",
    "#     tokens += tokenize(label, line)\n",
    "\n",
    "for a in train_dataset:\n",
    "    print(a)\n",
    "    print(type(a))\n",
    "    break\n",
    "\n",
    "# Initialize training_data\n",
    "# training_data = []\n",
    "\n",
    "# for example in train_dataset:\n",
    "#     source = example.context\n",
    "#     target = example.question\n",
    "#     training_data.append({\"source\": source, \"target\": target})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': ['To',\n",
       "   'whom',\n",
       "   'did',\n",
       "   'the',\n",
       "   'Virgin',\n",
       "   'Mary',\n",
       "   'allegedly',\n",
       "   'appear',\n",
       "   'in',\n",
       "   '1858',\n",
       "   'in',\n",
       "   'Lourdes',\n",
       "   'France',\n",
       "   '?'],\n",
       "  'target': ['Saint', 'Bernadette', 'Soubirous']}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
